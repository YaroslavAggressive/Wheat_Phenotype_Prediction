{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqeQ0cHQnCbv",
        "outputId": "4f166bdc-9c38-413e-81cd-de3754cf4dc6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorflow_decision_forests\n",
            "  Downloading tensorflow_decision_forests-1.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.5/15.5 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorflow_decision_forests) (1.25.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from tensorflow_decision_forests) (2.0.3)\n",
            "Collecting tensorflow~=2.16.1 (from tensorflow_decision_forests)\n",
            "  Downloading tensorflow-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (589.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m589.8/589.8 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from tensorflow_decision_forests) (1.16.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from tensorflow_decision_forests) (1.4.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from tensorflow_decision_forests) (0.43.0)\n",
            "Collecting wurlitzer (from tensorflow_decision_forests)\n",
            "  Downloading wurlitzer-3.1.0-py3-none-any.whl (8.4 kB)\n",
            "Collecting tf-keras~=2.16 (from tensorflow_decision_forests)\n",
            "  Downloading tf_keras-2.16.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ydf (from tensorflow_decision_forests)\n",
            "  Downloading ydf-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.2/9.2 MB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.16.1->tensorflow_decision_forests) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.16.1->tensorflow_decision_forests) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.16.1->tensorflow_decision_forests) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.16.1->tensorflow_decision_forests) (0.2.0)\n",
            "Collecting h5py>=3.10.0 (from tensorflow~=2.16.1->tensorflow_decision_forests)\n",
            "  Downloading h5py-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m55.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.16.1->tensorflow_decision_forests) (18.1.1)\n",
            "Collecting ml-dtypes~=0.3.1 (from tensorflow~=2.16.1->tensorflow_decision_forests)\n",
            "  Downloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.16.1->tensorflow_decision_forests) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.16.1->tensorflow_decision_forests) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.16.1->tensorflow_decision_forests) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.16.1->tensorflow_decision_forests) (2.31.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.16.1->tensorflow_decision_forests) (67.7.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.16.1->tensorflow_decision_forests) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.16.1->tensorflow_decision_forests) (4.11.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.16.1->tensorflow_decision_forests) (1.14.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.16.1->tensorflow_decision_forests) (1.64.0)\n",
            "Collecting tensorboard<2.17,>=2.16 (from tensorflow~=2.16.1->tensorflow_decision_forests)\n",
            "  Downloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras>=3.0.0 (from tensorflow~=2.16.1->tensorflow_decision_forests)\n",
            "  Downloading keras-3.3.3-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.16.1->tensorflow_decision_forests) (0.37.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->tensorflow_decision_forests) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->tensorflow_decision_forests) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->tensorflow_decision_forests) (2024.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow~=2.16.1->tensorflow_decision_forests) (13.7.1)\n",
            "Collecting namex (from keras>=3.0.0->tensorflow~=2.16.1->tensorflow_decision_forests)\n",
            "  Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
            "Collecting optree (from keras>=3.0.0->tensorflow~=2.16.1->tensorflow_decision_forests)\n",
            "  Downloading optree-0.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (311 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow~=2.16.1->tensorflow_decision_forests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow~=2.16.1->tensorflow_decision_forests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow~=2.16.1->tensorflow_decision_forests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow~=2.16.1->tensorflow_decision_forests) (2024.2.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow~=2.16.1->tensorflow_decision_forests) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow~=2.16.1->tensorflow_decision_forests) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow~=2.16.1->tensorflow_decision_forests) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow~=2.16.1->tensorflow_decision_forests) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.0.0->tensorflow~=2.16.1->tensorflow_decision_forests) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.0.0->tensorflow~=2.16.1->tensorflow_decision_forests) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow~=2.16.1->tensorflow_decision_forests) (0.1.2)\n",
            "Installing collected packages: namex, ydf, wurlitzer, optree, ml-dtypes, h5py, tensorboard, keras, tensorflow, tf-keras, tensorflow_decision_forests\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.2.0\n",
            "    Uninstalling ml-dtypes-0.2.0:\n",
            "      Successfully uninstalled ml-dtypes-0.2.0\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.9.0\n",
            "    Uninstalling h5py-3.9.0:\n",
            "      Successfully uninstalled h5py-3.9.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.15.2\n",
            "    Uninstalling tensorboard-2.15.2:\n",
            "      Successfully uninstalled tensorboard-2.15.2\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.15.0\n",
            "    Uninstalling keras-2.15.0:\n",
            "      Successfully uninstalled keras-2.15.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.15.0\n",
            "    Uninstalling tensorflow-2.15.0:\n",
            "      Successfully uninstalled tensorflow-2.15.0\n",
            "  Attempting uninstall: tf-keras\n",
            "    Found existing installation: tf_keras 2.15.1\n",
            "    Uninstalling tf_keras-2.15.1:\n",
            "      Successfully uninstalled tf_keras-2.15.1\n",
            "Successfully installed h5py-3.11.0 keras-3.3.3 ml-dtypes-0.3.2 namex-0.0.8 optree-0.11.0 tensorboard-2.16.2 tensorflow-2.16.1 tensorflow_decision_forests-1.9.1 tf-keras-2.16.0 wurlitzer-3.1.0 ydf-0.4.3\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (3.3.3)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras) (1.25.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.7.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.11.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras) (0.11.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras) (0.3.2)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras) (4.11.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.25.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow_decision_forests\n",
        "!pip install numpy\n",
        "!pip install pandas\n",
        "!pip install keras\n",
        "!pip install scikit-learn\n",
        "!pip install opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DF36u6uEwBMB"
      },
      "outputs": [],
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import Sequential, Model\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers import Concatenate\n",
        "from keras.layers import GlobalAveragePooling2D\n",
        "from keras.layers import Conv2DTranspose\n",
        "from keras.layers import Resizing\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow_decision_forests as tfdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "rE7GAxmts9cC"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.impute import KNNImputer\n",
        "\n",
        "def pca_features(data: np.ndarray, n_components: int = 10) -> np.ndarray:\n",
        "    flattened_data = np.array([img.flatten() for img in data])\n",
        "    data_processed = PCA(n_components=n_components).fit_transform(flattened_data)\n",
        "    return data_processed\n",
        "\n",
        "\n",
        "def t_sne_features(data: np.ndarray, n_components: int = 10):\n",
        "    data_embeded = TSNE(n_components=n_components,\n",
        "                        learning_rate='auto',\n",
        "                        init='random',\n",
        "                        method='exact',\n",
        "                        perplexity=3).fit_transform(data)\n",
        "    print(data_embeded.shape)\n",
        "    return data_embeded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "EAlpGeePtNK0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2 as cv\n",
        "import pandas as pd\n",
        "\n",
        "def load_images_from_folder(folder: str) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Функция подгрузки необходимого набора искусственных изображений из передаваемого каталога.\n",
        "\n",
        "    :param folder: папка с изображениями, сохраненными в формате .png\n",
        "\n",
        "    :return: список формата Numpy, содержащие AIO в объектах класса Image из Pillow\n",
        "    \"\"\"\n",
        "\n",
        "    images = []\n",
        "    for filename in os.listdir(folder):\n",
        "        img = cv.imread(os.path.join(folder, filename), cv.IMREAD_GRAYSCALE)\n",
        "        if img is not None:\n",
        "            images.append(np.asarray(img).astype(np.float32))\n",
        "    return np.asarray(images)\n",
        "\n",
        "folder_images = \"/content\"\n",
        "images = load_images_from_folder(folder_images)\n",
        "pca_features_ = pca_features(images, n_components=30)\n",
        "df_wheat = pd.read_csv(\"/content/wheat_pheno_num_sync.csv\")\n",
        "labels = df_wheat[[\"Урожайность.зерна..г.\", \"Высота.растений..см\"]].to_numpy()\n",
        "\n",
        "# импутирование данных\n",
        "# (Пока просто средними значениями) импутируем данные, поскольку присутствуют пропуски\n",
        "# imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "imp = KNNImputer(n_neighbors=2, weights='uniform')\n",
        "labels = imp.fit_transform(labels.reshape(-1, 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6sy6suYiw6hI"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "# Модель, скомбинированная со случайным лесом\n",
        "@dataclass\n",
        "class ComboModel:\n",
        "\n",
        "    n_epochs: int = 100\n",
        "    n_row: int = 200\n",
        "    n_col: int = 200\n",
        "    input_channels: int = 1\n",
        "    n_data: int = 100  # len(aio_labels)\n",
        "    random_seed: int = 1234567890\n",
        "    n_dict_features: int = 30\n",
        "    n_trait: int = 1\n",
        "    data_train: np.ndarray = np.ndarray([])\n",
        "    label_train: np.ndarray = np.ndarray([])\n",
        "\n",
        "    optimizer: keras.optimizers.Optimizer = None\n",
        "    model: keras.models.Model = None\n",
        "\n",
        "    def combo_model_functional(self, hp):\n",
        "        \"\"\"\n",
        "        Функция построения модели нейросети с функциональным интерфейсом keras\n",
        "\n",
        "        :param hp: набор гиперпараметров, отвечающих за конфигурация нейросети\n",
        "        :return: граф-представление нейросети\n",
        "        \"\"\"\n",
        "\n",
        "        inp_node = Input((self.n_row, self.n_col, self.input_channels), name=\"img_input\")\n",
        "        inp_dict_model = Input(self.n_dict_features, name=\"pop_struct_input\")\n",
        "\n",
        "        conv_node_1 = Conv2D(hp['first_conv2d_out_channels'],\n",
        "                             kernel_size=(hp['first_conv2d_kernel_size'], hp['first_conv2d_kernel_size']),\n",
        "                             padding='same',\n",
        "                             strides=(1, 1),\n",
        "                             activation=hp['first_conv2d_activation'], name=\"conv_map_1\")(inp_node)\n",
        "        if hp['need_extra_conv2d']:\n",
        "            conv_node_1 = Conv2D(hp['extra_conv2d_out_channels'],\n",
        "                                 kernel_size=(hp['extra_conv2d_kernel_size'], hp['extra_conv2d_kernel_size']),\n",
        "                                 padding='same',\n",
        "                                 strides=(1, 1),\n",
        "                                 activation=hp['extra_conv2d_activation'], name=\"conv_map_extra\")(conv_node_1)\n",
        "\n",
        "        if hp['need_batch_norm_after_first_conv2d']:\n",
        "            batch_node_1 = BatchNormalization()(conv_node_1)\n",
        "            mp_node_1 = MaxPooling2D(pool_size=(2, 2))(batch_node_1)\n",
        "        else:\n",
        "            mp_node_1 = MaxPooling2D(pool_size=(2, 2))(conv_node_1)\n",
        "\n",
        "        conv_node_2 = Conv2D(hp['second_conv2d_out_channels'],\n",
        "                             kernel_size=(hp['second_conv2d_kernel_size'], hp['second_conv2d_kernel_size']),\n",
        "                             padding='same',\n",
        "                             strides=(1, 1),\n",
        "                             activation=hp['second_conv2d_activation'], name=\"conv_map_2\")(mp_node_1)\n",
        "\n",
        "        if hp['need_batch_norm_after_second_conv2d']:\n",
        "            batch_node_2 = BatchNormalization()(conv_node_2)\n",
        "            mp_node_2 = MaxPooling2D(pool_size=(2, 2), name=\"max_pool_map\")(batch_node_2)\n",
        "        else:\n",
        "            mp_node_2 = MaxPooling2D(pool_size=(2, 2), name=\"max_pool_map\")(conv_node_2)\n",
        "\n",
        "        if hp['need_deconv_block']:\n",
        "            deconv_node_2 = Conv2DTranspose(\n",
        "                hp['second_conv2d_out_channels'],\n",
        "                kernel_size=(hp['second_conv2d_kernel_size'], hp['second_conv2d_kernel_size']),\n",
        "                padding='same',\n",
        "                strides=(2, 2),\n",
        "                activation=hp['second_conv2d_activation'],\n",
        "                name=\"deconv_2\"\n",
        "            )(mp_node_2)\n",
        "            concat_node_2 = Concatenate(name=\"concat_2\", axis=3)([deconv_node_2, conv_node_2])\n",
        "            conv_node_deconv_2 = Conv2D(\n",
        "                hp['second_conv2d_out_channels'],\n",
        "                kernel_size=(hp['second_conv2d_kernel_size'], hp['second_conv2d_kernel_size']),\n",
        "                padding='same',\n",
        "                strides=(1, 1),\n",
        "                activation=hp['second_conv2d_activation'],\n",
        "                name=\"conv_deconv_2\"\n",
        "            )(concat_node_2)\n",
        "            deconv_node_1 = Conv2DTranspose(\n",
        "                hp['first_conv2d_out_channels'],\n",
        "                kernel_size=(hp['first_conv2d_kernel_size'], hp['first_conv2d_kernel_size']),\n",
        "                padding='same',\n",
        "                strides=(2, 2),\n",
        "                activation=hp['first_conv2d_activation'],\n",
        "                name=\"deconv_1\"\n",
        "            )(conv_node_deconv_2)\n",
        "            concat_node_1 = Concatenate(name=\"concat_1\", axis=3)([deconv_node_1, conv_node_1])\n",
        "            mp_node_2 = Conv2D(\n",
        "                hp['first_conv2d_out_channels'],\n",
        "                kernel_size=(hp['first_conv2d_kernel_size'], hp['first_conv2d_kernel_size']),\n",
        "                padding='same',\n",
        "                strides=(1, 1),\n",
        "                activation=hp['first_conv2d_activation'],\n",
        "                name=\"conv_deconv_1\"\n",
        "            )(concat_node_1)\n",
        "\n",
        "        if hp['use_gap_1_or_flatten_0'] == 0:\n",
        "            flatten_node = Flatten(name='flatten')(mp_node_2)\n",
        "            dense_node = Dense(hp['num_feature_output'], activation=hp['dense_output_activation'],\n",
        "                               name=\"img_feature_output\")(flatten_node)\n",
        "        elif hp['use_gap_1_or_flatten_0'] == 1:\n",
        "            dense_node = GlobalAveragePooling2D(name=\"img_feature_output\")(mp_node_2)\n",
        "\n",
        "        concatenate_features = Concatenate(name=\"concat_features\")(inp_dict_model, dense_node)\n",
        "\n",
        "        reg_forest_1 = tfdf.keras.RandomForestModel(task=tfdf.keras.Task.REGRESSION,\n",
        "                                                    num_trees=hp['num_estimators'],\n",
        "                                                    max_depth=hp['max_depth'],\n",
        "                                                    bootstrap_training_dataset=hp['bootstrap'])\n",
        "        forest_1_pred = reg_forest_1(concatenate_features)\n",
        "\n",
        "        reg_forest_2 = tfdf.keras.RandomForestModel(task=tfdf.keras.Task.REGRESSION,\n",
        "                                                    num_trees=hp['num_estimators'],\n",
        "                                                    max_depth=hp['max_depth'],\n",
        "                                                    bootstrap_training_dataset=hp['bootstrap'])\n",
        "        forest_2_pred = reg_forest_2(concatenate_features)\n",
        "\n",
        "        combo_model = Model(inputs=[inp_node, inp_dict_model], outputs=[forest_1_pred, forest_2_pred],\n",
        "                            name=\"feature_model\")\n",
        "\n",
        "        self.model = combo_model\n",
        "\n",
        "    def build(self, hp):\n",
        "        \"\"\"\n",
        "        Builds a convolutional model.\n",
        "        \"\"\"\n",
        "        # Гиперпараметры сверточной части модели\n",
        "        model_hp = {\n",
        "            # сначала идут параметры сверточной части модели\n",
        "            'first_conv2d_out_channels': [32, 64],\n",
        "            'first_conv2d_kernel_size': [3, 5, 7],\n",
        "            'first_conv2d_activation': ['tanh', 'relu'],\n",
        "            'need_extra_conv2d': [False, True],\n",
        "            'extra_conv2d_out_channels': [32, 64],\n",
        "            'extra_conv2d_kernel_size': [3, 5, 7],\n",
        "            'extra_conv2d_activation': ['tanh', 'relu'],\n",
        "            'need_batch_norm_after_first_conv2d': [True, False],\n",
        "            'second_conv2d_kernel_size': [3, 5],\n",
        "            'second_conv2d_out_channels': [128, 64],\n",
        "            'second_conv2d_activation': ['tanh', 'relu'],\n",
        "            'need_batch_norm_after_second_conv2d': [True, False],\n",
        "            'dense_output_activation': ['sigmoid', 'linear'],\n",
        "            'use_gap_1_or_flatten_0': [1, 0],\n",
        "            'need_deconv_block': [False, True],\n",
        "            'num_feature_output': [128, 64, 256],\n",
        "            # а дальше идут параметры регрессионного случайного леса\n",
        "            'n_estimators': [5, 20, 50, 100],\n",
        "            'max_features': ['auto', 'sqrt'],\n",
        "            'max_depth': [(i + 1) * 5 for i in range(7)],\n",
        "            'min_samples_split': [2, 6, 10],\n",
        "            'bootstrap': [True, False]\n",
        "        }\n",
        "\n",
        "        # возвращаем собранную модель\n",
        "        return self.combo_model_functional(model_hp)\n",
        "\n",
        "    @staticmethod\n",
        "    @tf.function\n",
        "    def custom_loss_mae(y_true: np.ndarray, y_pred: np.ndarray) -> np.array:\n",
        "        error = y_true - y_pred\n",
        "        abs_error_1, abs_error_2 = tf.abs(error[:, 0]), tf.abs(error[:, 1])\n",
        "        result_1, result_2 = tf.reduce_mean(abs_error_1), tf.reduce_mean(abs_error_2)\n",
        "        # return np.array([result_1, result_2])\n",
        "\n",
        "        return np.sqrt(result_1 * result_2)\n",
        "\n",
        "    @staticmethod\n",
        "    @tf.function\n",
        "    def custom_loss_mse(y_true: np.ndarray, y_pred: np.ndarray) -> np.array:\n",
        "        error = y_true - y_pred\n",
        "        squared_error_1, squared_error_2 = tf.square(error), tf.square(error[:, 1])\n",
        "        result_1, result_2 = tf.reduce_mean(squared_error_1), tf.reduce_mean(squared_error_2)\n",
        "        # return np.array([result_1, result_2])\n",
        "        return np.sqrt(result_1 * result_2)\n",
        "\n",
        "    # Function to run the train step.\n",
        "    # здесь надо подумать как исправить эту функцию\n",
        "    @tf.function\n",
        "    def run_train_step(self, images_, pop_comps_, labels_1_, labels_2_):\n",
        "        with tf.GradientTape() as tape:\n",
        "            logits_1, logits_2 = self.model(images_, pop_comps_)\n",
        "            loss_1 = loss_fn(labels, logits_1)\n",
        "            loss_2 = loss_fn(labels, logits_2)\n",
        "            # Add any regularization losses.\n",
        "            if self.model.losses:\n",
        "                loss_1 += tf.math.add_n(self.model.losses)\n",
        "                loss_2 += tf.math.add_n(self.model.losses)\n",
        "        gradients = tape.gradient(loss_1, self.model.trainable_variables)\n",
        "        gradients = tape.gradient(loss_2, self.model.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
        "\n",
        "    # Function to run the validation step.\n",
        "    @tf.function\n",
        "    def run_val_step(self, images_, pop_comps_, labels_1_, labels_2_):\n",
        "        logits = self.model(images)\n",
        "        loss = loss_fn(labels, logits)\n",
        "        # Update the metric.\n",
        "        epoch_loss_metric.update_state(loss)\n",
        "\n",
        "    @staticmethod\n",
        "    def fit_cv(comp_hp: dict, model_hp: dict, model: 'ComboModel', splits_num: int = 10) -> list:\n",
        "        model_keras = model.build(model_hp)\n",
        "\n",
        "        model_keras.compile(optimizer=keras.optimizers.SGD(learning_rate=0.0001),\n",
        "                            loss=ComboModelTuner.custom_loss_mae,\n",
        "                            loss_weights=1.0,\n",
        "                            metrics=[ComboModelTuner.custom_loss_mse])\n",
        "\n",
        "        mae_per_fold_tr, mse_per_fold_tr = [], []\n",
        "        mae_per_fold_vd, mse_per_fold_vd = [], []\n",
        "\n",
        "        kfold = KFold(n_splits=splits_num, shuffle=True)\n",
        "        for j, (tr_idx, val_idx) in enumerate(kfold.split(model.features_train, model.data_train, model.labels_train)):\n",
        "            model_keras.fit(x=[model.data_train[tr_idx], model.features_train[tr_idx]],\n",
        "                            y=model.labels_train[tr_idx],\n",
        "                            batch_size=total_hp[\"batch_size_ll\"],\n",
        "                            epochs=total_hp[\"num_epochs_ll\"])\n",
        "\n",
        "            scores = model_keras.evaluate(x=[model.data_train[tr_idx], model.features_train[tr_idx]],\n",
        "                                          y=model.labels_train[tr_idx])\n",
        "\n",
        "            mse_per_fold_tr.append(scores[0])\n",
        "            mae_per_fold_tr.append(scores[1])\n",
        "\n",
        "            scores = model_keras.evaluate(x=[model.data_train[val_idx], model.features_train[val_idx]],\n",
        "                                          y=model.labels_train[val_idx])\n",
        "\n",
        "            mse_per_fold_vd.append(scores[0])\n",
        "            mae_per_fold_vd.append(scores[1])\n",
        "            print(f\"Fold #{j + 1} finished succesfully\")\n",
        "\n",
        "        return [mae_per_fold_tr, mse_per_fold_tr, mae_per_fold_vd, mse_per_fold_vd]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "P-WRrNLWo57I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "6f99704d-cb89-4ed1-9011-02da0a70bda4"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unexpected indent (<ipython-input-6-fe16b81e7eef>, line 47)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-fe16b81e7eef>\"\u001b[0;36m, line \u001b[0;32m47\u001b[0m\n\u001b[0;31m    metrics = ComboModelTuner.custom_cv(total_hp={\"batch_size_ll\": 64, \"num_epochs_ll\": 30},\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ],
      "source": [
        "# обучение модели со случайным лесом\n",
        "\n",
        "def hyper_tuning(model: 'ComboModel', iters_num: int, hps_cnn: dict, hps_reg: dict):\n",
        "    valid_mae_label_1, valid_mae_label_2 = [], []\n",
        "    valid_mse_label_1, valid_mse_label_2 = [], []\n",
        "    valid_accuracy_label_1, valid_accuracy_label_2 = [], []\n",
        "\n",
        "    for iter_ in range(iters_num):\n",
        "        # Сборка комбинации случайных гиперпараметров в заданын границах\n",
        "        cnn_hp_comb, reg_hp_comb = {}, {}\n",
        "\n",
        "        for param in hps_cnn:\n",
        "            if len(hps_cnn[param]) > 1:\n",
        "                if any(isinstance(x, bool) for x in hps_cnn[param]) or any(isinstance(x, str) for x in hps_cnn[param]):\n",
        "                    cnn_hp_comb[param] = hps_cnn[param][np.random.randint(len(hps_cnn[param]))]\n",
        "                else:\n",
        "                    cnn_hp_comb[param] = np.random.randint(low=min(hps_cnn[param]), high=max(hps_cnn[param]))\n",
        "            else:\n",
        "                cnn_hp_comb[param] = hps_cnn[param][0]\n",
        "\n",
        "        for param in hps_reg:\n",
        "            if len(hps_reg[param]) > 1:\n",
        "                if any(isinstance(x, bool) for x in hps_reg[param]) or any(isinstance(x, str) for x in hps_reg[param]):\n",
        "                    reg_hp_comb[param] = hps_reg[param][np.random.randint(len(hps_reg[param]))]\n",
        "                else:\n",
        "                    reg_hp_comb[param] = np.random.randint(low=min(hps_reg[param]), high=max(hps_reg[param]))\n",
        "            else:\n",
        "                reg_hp_comb[param] = hps_reg[param][0]\n",
        "\n",
        "        print(cnn_hp_comb)\n",
        "\n",
        "        model = SimpleCNNModel(n_epochs=20,\n",
        "                               n_row=200,\n",
        "                               n_col=200,\n",
        "                               input_channels=1,\n",
        "                               random_seed=1234567890,\n",
        "                               n_dict_features=30,\n",
        "                               n_trait=2,\n",
        "                               data_train=train_images_,\n",
        "                               labels_train=train_labels_,\n",
        "                               features_train=train_features_,\n",
        "                               data_test=test_images_,\n",
        "                               features_test=test_features_,\n",
        "                               labels_test=test_labels_)\n",
        "\n",
        "            # model = SimpleCNNModel()\n",
        "          metrics = ComboModelTuner.custom_cv(total_hp={\"batch_size_ll\": 64, \"num_epochs_ll\": 30},\n",
        "                                              cnn_hp=cnn_hp_comb,\n",
        "                                              model=model)\n",
        "\n",
        "        # считаем ошибку модели на тестовой выборке\n",
        "        print(f\"Random Tuning iter #{iter_} finished successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "B2tXgRld2Sie"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "# Модель сугубо нейронной сети\n",
        "@dataclass\n",
        "class SimpleCNNModel:\n",
        "    n_epochs: int = 20\n",
        "    n_row: int = 200\n",
        "    n_col: int = 200\n",
        "    input_channels: int = 1\n",
        "    random_seed: int = 1234567890\n",
        "    n_dict_features: int = 30\n",
        "    n_trait: int = 2\n",
        "\n",
        "    data_train: np.ndarray = np.ndarray([])\n",
        "    features_train: np.ndarray = np.asarray([])\n",
        "    labels_train: np.ndarray = np.ndarray([])\n",
        "\n",
        "    data_test: np.ndarray = np.asarray([])\n",
        "    features_test: np.ndarray = np.asarray([])\n",
        "    labels_test: np.ndarray = np.asarray([])\n",
        "\n",
        "    def build(self, hp: dict):\n",
        "        \"\"\"\n",
        "        Функция построения модели нейросети с функциональным интерфейсом keras\n",
        "\n",
        "        :param hp: набор гиперпараметров, отвечающих за конфигурация нейросети\n",
        "        :return: граф-представление нейросети\n",
        "        \"\"\"\n",
        "\n",
        "        inp_node = Input((self.n_row, self.n_col, self.input_channels), name=\"img_input\")\n",
        "\n",
        "        inp_node_dict = Input({self.n_dict_features}, name=\"dict_input\")\n",
        "\n",
        "        conv_node_1 = Conv2D(hp['first_conv2d_out_channels'],\n",
        "                             kernel_size=(hp['first_conv2d_kernel_size'], hp['first_conv2d_kernel_size']),\n",
        "                             padding='same',\n",
        "                             strides=(1, 1),\n",
        "                             activation=hp['first_conv2d_activation'], name=\"conv_map_1\")(inp_node)\n",
        "        if hp['need_extra_conv2d']:\n",
        "            conv_node_1 = Conv2D(hp['extra_conv2d_out_channels'],\n",
        "                                 kernel_size=(hp['extra_conv2d_kernel_size'], hp['extra_conv2d_kernel_size']),\n",
        "                                 padding='same',\n",
        "                                 strides=(1, 1),\n",
        "                                 activation=hp['extra_conv2d_activation'], name=\"conv_map_extra\")(conv_node_1)\n",
        "\n",
        "        if hp['need_batch_norm_after_first_conv2d']:\n",
        "            batch_node_1 = BatchNormalization()(conv_node_1)\n",
        "            mp_node_1 = MaxPooling2D(pool_size=(2, 2))(batch_node_1)\n",
        "        else:\n",
        "            mp_node_1 = MaxPooling2D(pool_size=(2, 2))(conv_node_1)\n",
        "\n",
        "        conv_node_2 = Conv2D(hp['second_conv2d_out_channels'],\n",
        "                             kernel_size=(hp['second_conv2d_kernel_size'], hp['second_conv2d_kernel_size']),\n",
        "                             padding='same',\n",
        "                             strides=(1, 1),\n",
        "                             activation=hp['second_conv2d_activation'], name=\"conv_map_2\")(mp_node_1)\n",
        "\n",
        "        if hp['need_batch_norm_after_second_conv2d']:\n",
        "            batch_node_2 = BatchNormalization()(conv_node_2)\n",
        "            mp_node_2 = MaxPooling2D(pool_size=(2, 2), name=\"max_pool_map\")(batch_node_2)\n",
        "        else:\n",
        "            mp_node_2 = MaxPooling2D(pool_size=(2, 2), name=\"max_pool_map\")(conv_node_2)\n",
        "\n",
        "        if hp['need_deconv_block']:\n",
        "            deconv_node_2 = Conv2DTranspose(\n",
        "                hp['second_conv2d_out_channels'],\n",
        "                kernel_size=(hp['second_conv2d_kernel_size'], hp['second_conv2d_kernel_size']),\n",
        "                padding='same',\n",
        "                strides=(2, 2),\n",
        "                activation=hp['second_conv2d_activation'],\n",
        "                name=\"deconv_2\"\n",
        "            )(mp_node_2)\n",
        "            concat_node_2 = Concatenate(name=\"concat_2\", axis=3)([deconv_node_2, conv_node_2])\n",
        "            conv_node_deconv_2 = Conv2D(\n",
        "                hp['second_conv2d_out_channels'],\n",
        "                kernel_size=(hp['second_conv2d_kernel_size'], hp['second_conv2d_kernel_size']),\n",
        "                padding='same',\n",
        "                strides=(1, 1),\n",
        "                activation=hp['second_conv2d_activation'],\n",
        "                name=\"conv_deconv_2\"\n",
        "            )(concat_node_2)\n",
        "            deconv_node_1 = Conv2DTranspose(\n",
        "                hp['first_conv2d_out_channels'],\n",
        "                kernel_size=(hp['first_conv2d_kernel_size'], hp['first_conv2d_kernel_size']),\n",
        "                padding='same',\n",
        "                strides=(2, 2),\n",
        "                activation=hp['first_conv2d_activation'],\n",
        "                name=\"deconv_1\"\n",
        "            )(conv_node_deconv_2)\n",
        "            concat_node_1 = Concatenate(name=\"concat_1\", axis=3)([deconv_node_1, conv_node_1])\n",
        "            mp_node_2 = Conv2D(\n",
        "                hp['first_conv2d_out_channels'],\n",
        "                kernel_size=(hp['first_conv2d_kernel_size'], hp['first_conv2d_kernel_size']),\n",
        "                padding='same',\n",
        "                strides=(1, 1),\n",
        "                activation=hp['first_conv2d_activation'],\n",
        "                name=\"conv_deconv_1\"\n",
        "            )(concat_node_1)\n",
        "\n",
        "        if hp['use_gap_1_or_flatten_0'] == 0:\n",
        "            flatten_node = Flatten(name='flatten')(mp_node_2)\n",
        "            dense_node = Dense(hp['num_feature_output'], activation=hp['dense_output_activation'],\n",
        "                               name=\"img_feature_output\")(flatten_node)\n",
        "        elif hp['use_gap_1_or_flatten_0'] == 1:\n",
        "            dense_node = GlobalAveragePooling2D(name=\"img_feature_output\")(mp_node_2)\n",
        "\n",
        "        concatenate_features = Concatenate(name=\"concat_features\")([inp_node_dict, dense_node])\n",
        "\n",
        "        out = Dense(self.n_trait, activation='linear', name=\"cnn_multioutput\")(concatenate_features)\n",
        "\n",
        "        model = Model(inputs=[inp_node, inp_node_dict], outputs=out, name=\"regression_model\")\n",
        "\n",
        "        return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Z5t8EVBx1_ZP"
      },
      "outputs": [],
      "source": [
        "class ComboDataPool(keras.utils.Sequence):\n",
        "\n",
        "    def __init__(self, images, features, labels, batch_size: int, max_len: int = -1):\n",
        "        self.batch_size = batch_size\n",
        "        self.images = images[:max_len]\n",
        "        self.features = features[:max_len]\n",
        "        self.labels = labels[:max_len]\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(self.images.shape[0] / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_data = [self.images[idx * self.batch_size:(idx + 1) * self.batch_size],\n",
        "                   self.features[idx * self.batch_size:(idx + 1) * self.batch_size]]\n",
        "        batch_labels = self.labels[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "\n",
        "        return batch_data, batch_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "bjUvQz9x2OgG"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "from sklearn.model_selection import KFold\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "\n",
        "class ComboModelTuner:\n",
        "\n",
        "    @staticmethod\n",
        "    @tf.function\n",
        "    def custom_loss_mae(y_true: np.ndarray, y_pred: np.ndarray) -> np.array:\n",
        "        error = y_true - y_pred\n",
        "        abs_error_1, abs_error_2 = tf.abs(error[:, 0]), tf.abs(error[:, 1])\n",
        "        result_1, result_2 = tf.reduce_mean(abs_error_1), tf.reduce_mean(abs_error_2)\n",
        "        return (result_1 + result_2) / 2\n",
        "\n",
        "    @staticmethod\n",
        "    @tf.function\n",
        "    def custom_loss_mse(y_true: np.ndarray, y_pred: np.ndarray) -> np.array:\n",
        "        error = y_true - y_pred\n",
        "        squared_error_1, squared_error_2 = tf.square(error), tf.square(error[:, 1])\n",
        "        result_1, result_2 = tf.reduce_mean(squared_error_1), tf.reduce_mean(squared_error_2)\n",
        "        return (result_1 + result_2) / 2\n",
        "\n",
        "    @staticmethod\n",
        "    def custom_cv(total_hp: dict, cnn_hp: dict, model: 'SimpleCNNModel', splits_num: int = 10,\n",
        "                  early_stop: bool = True, model_checkpoint: bool = True) -> list:\n",
        "        model_keras = model.build(cnn_hp)\n",
        "\n",
        "        callbacks = []\n",
        "        if early_stop:\n",
        "            callback_early_stop = EarlyStopping(monitor=\"loss\", min_delta=0.001, patience=2, verbose=1)\n",
        "            callbacks.append(callback_early_stop)\n",
        "        if model_checkpoint:\n",
        "            callback_checkpoint = ModelCheckpoint(filepath=\"checkpoints/model_no_df_{epoch}.keras\",\n",
        "                                                  save_best_only=True, monitor=\"loss\", verbose=1)\n",
        "            callbacks.append(callback_checkpoint)\n",
        "\n",
        "        model_keras.compile(optimizer=keras.optimizers.SGD(learning_rate=0.01),\n",
        "                            loss=ComboModelTuner.custom_loss_mae,\n",
        "                            loss_weights=1.0,\n",
        "                            metrics=[ComboModelTuner.custom_loss_mae, ComboModelTuner.custom_loss_mse])\n",
        "\n",
        "        learning_data_pool = ComboDataPool(images=train_images,\n",
        "                                           features=train_dict,\n",
        "                                           labels=train_labels,\n",
        "                                           batch_size=64)\n",
        "\n",
        "        mae_per_fold_tr, mse_per_fold_tr = [], []\n",
        "        mae_per_fold_vd, mse_per_fold_vd = [], []\n",
        "\n",
        "        kfold = KFold(n_splits=splits_num, shuffle=True)\n",
        "        for j, (tr_idx, val_idx) in enumerate(kfold.split(model.features_train, model.data_train, model.labels_train)):\n",
        "            model_keras.fit(x=[model.data_train[tr_idx], model.features_train[tr_idx]],\n",
        "                            y=model.labels_train[tr_idx],\n",
        "                            batch_size=total_hp[\"batch_size_ll\"],\n",
        "                            epochs=total_hp[\"num_epochs_ll\"],\n",
        "                            validation_data=([model.data_train[val_idx], model.features_train[val_idx]],\n",
        "                                             model.labels_train[val_idx]))\n",
        "\n",
        "            # model_keras.fit(learning_data_pool, epochs=total_hp[\"num_epochs_ll\"],\n",
        "            #                 validation_data=([model.data_train[val_idx], model.features_train[val_idx]],\n",
        "            #                                  model.labels_train[val_idx]))\n",
        "\n",
        "            scores = model_keras.evaluate(x=[model.data_train[tr_idx], model.features_train[tr_idx]],\n",
        "                                          y=model.labels_train[tr_idx])\n",
        "\n",
        "            mse_per_fold_tr.append(scores[0])\n",
        "            mae_per_fold_tr.append(scores[1])\n",
        "\n",
        "            scores = model_keras.evaluate(x=[model.data_train[val_idx], model.features_train[val_idx]],\n",
        "                                          y=model.labels_train[val_idx])\n",
        "\n",
        "            mse_per_fold_vd.append(scores[0])\n",
        "            mae_per_fold_vd.append(scores[1])\n",
        "            print(f\"Fold #{j + 1} finished succesfully\")\n",
        "\n",
        "        return [mae_per_fold_tr, mse_per_fold_tr, mae_per_fold_vd, mse_per_fold_vd]\n",
        "\n",
        "    @staticmethod\n",
        "    def fit_cv(total_hp: dict, cnn_hp: dict, model: 'SimpleCNNModel', splits_num: int = 10,\n",
        "               early_stop: bool = True, model_checkpoint: bool = True) -> list:\n",
        "        model_keras = model.build(cnn_hp)\n",
        "\n",
        "        learning_data_pool = ComboDataPool(images=train_images,\n",
        "                                           features=train_dict,\n",
        "                                           labels=train_labels,\n",
        "                                           batch_size=64)\n",
        "\n",
        "        callbacks = []\n",
        "        if early_stop:\n",
        "            callback_early_stop = EarlyStopping(monitor=\"loss\", min_delta=0.001, patience=2, verbose=1)\n",
        "            callbacks.append(callback_early_stop)\n",
        "        if model_checkpoint:\n",
        "            callback_checkpoint = ModelCheckpoint(filepath=\"checkpoints/model_no_df_{epoch}.keras\",\n",
        "                                                  save_best_only=True, monitor=\"loss\", verbose=1)\n",
        "            callbacks.append(callback_checkpoint)\n",
        "\n",
        "        model_keras.compile(optimizer=keras.optimizers.SGD(learning_rate=0.01),\n",
        "                            loss=ComboModelTuner.custom_loss_mae,\n",
        "                            loss_weights=1.0,\n",
        "                            metrics=[ComboModelTuner.custom_loss_mae, ComboModelTuner.custom_loss_mse])\n",
        "\n",
        "        mae_per_fold_tr, mse_per_fold_tr = [], []\n",
        "        mae_per_fold_vd, mse_per_fold_vd = [], []\n",
        "\n",
        "        kfold = KFold(n_splits=splits_num, shuffle=True)\n",
        "        for j, (tr_idx, val_idx) in enumerate(kfold.split(model.features_train, model.data_train, model.labels_train)):\n",
        "            model_keras.fit(x=[model.data_train[tr_idx], model.features_train[tr_idx]],\n",
        "                            y=model.labels_train[tr_idx],\n",
        "                            batch_size=total_hp[\"batch_size_ll\"],\n",
        "                            epochs=total_hp[\"num_epochs_ll\"],\n",
        "                            validation_data=([model.data_train[val_idx], model.features_train[val_idx]],\n",
        "                                             model.labels_train[val_idx]))\n",
        "\n",
        "            # model_keras.fit(learning_data_pool, epochs=total_hp[\"num_epochs_ll\"],\n",
        "                            # validation_data=([model.data_train[val_idx], model.features_train[val_idx]],\n",
        "                                            #  model.labels_train[val_idx]))\n",
        "\n",
        "            scores = model_keras.evaluate(x=[model.data_train[tr_idx], model.features_train[tr_idx]],\n",
        "                                          y=model.labels_train[tr_idx])\n",
        "\n",
        "            mse_per_fold_tr.append(scores[0])\n",
        "            mae_per_fold_tr.append(scores[1])\n",
        "\n",
        "            scores = model_keras.evaluate(x=[model.data_train[val_idx], model.features_train[val_idx]],\n",
        "                                          y=model.labels_train[val_idx])\n",
        "\n",
        "            mse_per_fold_vd.append(scores[0])\n",
        "            mae_per_fold_vd.append(scores[1])\n",
        "            print(f\"Fold #{j + 1} finished succesfully\")\n",
        "\n",
        "        return [mae_per_fold_tr, mse_per_fold_tr, mae_per_fold_vd, mse_per_fold_vd]\n",
        "\n",
        "    @staticmethod\n",
        "    def random_hyper_tuning(iters_num: int, hps_cnn: dict,\n",
        "                            train_images_: np.ndarray, train_features_: np.ndarray, train_labels_: np.ndarray,\n",
        "                            test_images_: np.ndarray, test_features_: np.ndarray, test_labels_: np.ndarray):\n",
        "\n",
        "        valid_mae_label_1, valid_mae_label_2 = [], []\n",
        "        valid_mse_label_1, valid_mse_label_2 = [], []\n",
        "        valid_accuracy_label_1, valid_accuracy_label_2 = [], []\n",
        "\n",
        "        for iter_ in range(iters_num):\n",
        "            # Сборка комбинации случайных гиперпараметров в заданын границах\n",
        "            print(f\"Random Tuning iter #{iter_} started\")\n",
        "\n",
        "            cnn_hp_comb = {}\n",
        "\n",
        "            for param in hps_cnn:\n",
        "                if len(hps_cnn[param]) > 1:\n",
        "                    if any(isinstance(x, bool) for x in hps_cnn[param]) or \\\n",
        "                            any(isinstance(x, str) for x in hps_cnn[param]):\n",
        "                        cnn_hp_comb[param] = hps_cnn[param][np.random.randint(len(hps_cnn[param]))]\n",
        "                    else:\n",
        "                        cnn_hp_comb[param] = np.random.randint(low=min(hps_cnn[param]), high=max(hps_cnn[param]))\n",
        "                else:\n",
        "                    cnn_hp_comb[param] = hps_cnn[param][0]\n",
        "\n",
        "            print(cnn_hp_comb)\n",
        "\n",
        "            model = SimpleCNNModel(n_epochs=20,\n",
        "                                   n_row=200,\n",
        "                                   n_col=200,\n",
        "                                   input_channels=1,\n",
        "                                   random_seed=1234567890,\n",
        "                                   n_dict_features=30,\n",
        "                                   n_trait=2,\n",
        "                                   data_train=train_images_,\n",
        "                                   labels_train=train_labels_,\n",
        "                                   features_train=train_features_,\n",
        "                                   data_test=test_images_,\n",
        "                                   features_test=test_features_,\n",
        "                                   labels_test=test_labels_)\n",
        "\n",
        "            # model = SimpleCNNModel()\n",
        "            metrics = ComboModelTuner.fit_cv(total_hp={\"batch_size_ll\": 64, \"num_epochs_ll\": 20},\n",
        "                                             cnn_hp=cnn_hp_comb,\n",
        "                                             splits_num=5,\n",
        "                                             model=model)\n",
        "\n",
        "            # считаем ошибку модели на тестовой выборке\n",
        "            print(f\"Random Tuning iter #{iter_} finished successfully\")\n",
        "\n",
        "    @staticmethod\n",
        "    def grid_hyper_tuning(model: 'SimpleCNNModel', hps_cnn: dict, hps_reg: dict):\n",
        "        cnn_hp_combos = itertools.product(*hps_cnn)\n",
        "        reg_hp_combos = itertools.product(*hps_reg)\n",
        "\n",
        "        valid_mae_label_1, valid_mae_label_2 = [], []\n",
        "        valid_mse_label_1, valid_mse_label_2 = [], []\n",
        "        valid_accuracy_label_1, valid_accuracy_label_2 = [], []\n",
        "\n",
        "        for i, tmp_hps_cnn in enumerate(cnn_hp_combos):\n",
        "            for j, tmp_hps_reg in enumerate(reg_hp_combos):\n",
        "                model.fit(dict(tmp_hps_cnn), dict(tmp_hps_reg))\n",
        "\n",
        "                # считаем ошибку модели на тестовой выборке\n",
        "                valid_predict = model.predict\n",
        "                print(f\"Grid Tuning iter #{i * len(reg_hp_combos) + j} finished successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "JU2Uemr52HkI"
      },
      "outputs": [],
      "source": [
        "# делим данные на обучение/валидацию/тест\n",
        "test_percentage = 0.1\n",
        "test_indices = np.random.choice(images.shape[0], int(images.shape[0] * test_percentage))\n",
        "train_indices = np.setdiff1d(np.array(list(range(images.shape[0]))), test_indices)\n",
        "\n",
        "train_images, train_labels, train_dict = images[train_indices], labels[train_indices], pca_features_[train_indices]\n",
        "test_images, test_labels, test_dict = images[test_indices], labels[test_indices], pca_features_[test_indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "FzfGemud2HsX",
        "outputId": "f00c4509-1a60-43e2-8929-46851d05a917"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Tuning iter #0 started\n",
            "{'first_conv2d_out_channels': 38, 'first_conv2d_kernel_size': 6, 'first_conv2d_activation': 'tanh', 'need_extra_conv2d': False, 'extra_conv2d_out_channels': 43, 'extra_conv2d_kernel_size': 4, 'extra_conv2d_activation': 'tanh', 'need_batch_norm_after_first_conv2d': False, 'second_conv2d_kernel_size': 3, 'second_conv2d_out_channels': 93, 'second_conv2d_activation': 'relu', 'need_batch_norm_after_second_conv2d': True, 'dense_output_activation': 'linear', 'use_gap_1_or_flatten_0': 0, 'need_deconv_block': True, 'num_feature_output': 204}\n",
            "Epoch 1/20\n",
            "\u001b[1m 6/11\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m7:42\u001b[0m 92s/step - custom_loss_mae: 101035.0547 - custom_loss_mse: 273925947392.0000 - loss: 101035.0547"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-f89af2011f45>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m         }\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m ComboModelTuner.random_hyper_tuning(10, model_hp,\n\u001b[0m\u001b[1;32m     23\u001b[0m                                     \u001b[0mtrain_images_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                                     \u001b[0mtrain_labels_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-bc6c8e4be5e4>\u001b[0m in \u001b[0;36mrandom_hyper_tuning\u001b[0;34m(iters_num, hps_cnn, train_images_, train_features_, train_labels_, test_images_, test_features_, test_labels_)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;31m# model = SimpleCNNModel()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m             metrics = ComboModelTuner.fit_cv(total_hp={\"batch_size_ll\": 64, \"num_epochs_ll\": 20},\n\u001b[0m\u001b[1;32m    177\u001b[0m                                              \u001b[0mcnn_hp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcnn_hp_comb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m                                              \u001b[0msplits_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-bc6c8e4be5e4>\u001b[0m in \u001b[0;36mfit_cv\u001b[0;34m(total_hp, cnn_hp, model, splits_num, early_stop, model_checkpoint)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mkfold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msplits_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtr_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_idx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkfold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             model_keras.fit(x=[model.data_train[tr_idx], model.features_train[tr_idx]],\n\u001b[0m\u001b[1;32m    109\u001b[0m                             \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtr_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_hp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"batch_size_ll\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    312\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pythonify_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1498\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1499\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1500\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1501\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1502\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# задаем сетку гиперпараметров для\n",
        "\n",
        "model_hp = {# сначала идут параметры сверточной части модели\n",
        "            'first_conv2d_out_channels': [32, 64],\n",
        "            'first_conv2d_kernel_size': [3, 5, 7],\n",
        "            'first_conv2d_activation': ['tanh', 'relu'],\n",
        "            'need_extra_conv2d': [False, True],\n",
        "            'extra_conv2d_out_channels': [32, 64],\n",
        "            'extra_conv2d_kernel_size': [3, 5, 7],\n",
        "            'extra_conv2d_activation': ['tanh', 'relu'],\n",
        "            'need_batch_norm_after_first_conv2d': [True, False],\n",
        "            'second_conv2d_kernel_size': [3, 5],\n",
        "            'second_conv2d_out_channels': [64, 128],\n",
        "            'second_conv2d_activation': ['tanh', 'relu'],\n",
        "            'need_batch_norm_after_second_conv2d': [True, False],\n",
        "            'dense_output_activation': ['sigmoid', 'linear'],\n",
        "            'use_gap_1_or_flatten_0': [1, 0],\n",
        "            'need_deconv_block': [False, True],\n",
        "            'num_feature_output': [64, 128, 256],\n",
        "        }\n",
        "\n",
        "ComboModelTuner.random_hyper_tuning(10, model_hp,\n",
        "                                    train_images_=train_images,\n",
        "                                    train_labels_=train_labels,\n",
        "                                    train_features_=train_dict,\n",
        "                                    test_images_=test_images,\n",
        "                                    test_labels_=test_labels,\n",
        "                                    test_features_=test_dict)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}