{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqeQ0cHQnCbv",
        "outputId": "5e6ef536-43e7-4345-d901-e7682c648a37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: umap-learn in /usr/local/lib/python3.10/dist-packages (0.5.6)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.2.2)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (0.58.1)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (0.5.12)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from umap-learn) (4.66.4)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.2->umap-learn) (0.41.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from pynndescent>=0.5->umap-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22->umap-learn) (3.5.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install umap-learn\n",
        "# !pip install numpy\n",
        "# !pip install pandas\n",
        "# !pip install keras\n",
        "# !pip install scikit-learn\n",
        "# !pip install opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DF36u6uEwBMB"
      },
      "outputs": [],
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import Sequential, Model\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers import Concatenate\n",
        "from keras.layers import GlobalAveragePooling2D\n",
        "from keras.layers import Conv2DTranspose\n",
        "from keras.layers import Resizing\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import numpy as np\n",
        "from scipy.stats import rankdata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "rE7GAxmts9cC"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.impute import KNNImputer\n",
        "import umap\n",
        "\n",
        "def pca_features(data: np.ndarray, n_components: int = 10) -> np.ndarray:\n",
        "    flattened_data = np.array([img.flatten() for img in data])\n",
        "    data_processed = PCA(n_components=n_components).fit_transform(flattened_data)\n",
        "    return data_processed\n",
        "\n",
        "\n",
        "def t_sne_features(data: np.ndarray, n_components: int = 10):\n",
        "    flattened_data = np.array([img.flatten() for img in data])\n",
        "    data_embeded = TSNE(n_components=n_components,\n",
        "                        learning_rate='auto',\n",
        "                        init='random',\n",
        "                        method='exact',\n",
        "                        perplexity=3).fit_transform(flattened_data)\n",
        "    return data_embeded\n",
        "\n",
        "def umap_features(data: np.ndarray, n_components: int = 10):\n",
        "    flattened_data = np.array([img.flatten() for img in data])\n",
        "    data_processed = umap.UMAP().fit_transform(flattened_data)\n",
        "    return data_processed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "c9uUnR0C8ZI3"
      },
      "outputs": [],
      "source": [
        "def rank_based_transform(x, k=0.5):\n",
        "    num_samp = np.sum(~np.isnan(x))\n",
        "    ranks = (rankdata(x, method='ordinal').astype(float) - k) / (num_samp - 2 * k + 1)\n",
        "    return np.log(ranks / (1 - ranks))\n",
        "\n",
        "\n",
        "def data_standardization(arr: np.ndarray) -> np.ndarray:\n",
        "    return (arr - arr.mean()) / arr.std()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "EAlpGeePtNK0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2 as cv\n",
        "import pandas as pd\n",
        "\n",
        "def load_images_from_folder(folder: str) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Функция подгрузки необходимого набора искусственных изображений из передаваемого каталога.\n",
        "\n",
        "    :param folder: папка с изображениями, сохраненными в формате .png\n",
        "\n",
        "    :return: список формата Numpy, содержащие AIO в объектах класса Image из Pillow\n",
        "    \"\"\"\n",
        "\n",
        "    images = []\n",
        "    for filename in os.listdir(folder):\n",
        "        img = cv.imread(os.path.join(folder, filename), cv.IMREAD_GRAYSCALE)\n",
        "        if img is not None:\n",
        "            images.append(np.asarray(img).astype(np.float32))\n",
        "    return np.asarray(images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ShEu8HAhfY1z"
      },
      "outputs": [],
      "source": [
        "n_pca, n_t_sne, n_umap = 5, 2, 5\n",
        "n_plants_use = 200\n",
        "\n",
        "folder_images = \"/content\"\n",
        "images = load_images_from_folder(folder_images)\n",
        "\n",
        "# выделение фич из изображений\n",
        "pca_features_ = pca_features(images, n_components=n_pca)\n",
        "t_sne_features_ = t_sne_features(images, n_components=n_t_sne)\n",
        "# umap_features_ = umap_features(images, n_components=n_umap)\n",
        "\n",
        "total_features = np.concatenate((pca_features_, t_sne_features_), axis=1)[:n_plants_use]\n",
        "\n",
        "df_wheat = pd.read_csv(\"/content/wheat_pheno_num_sync.csv\")[:n_plants_use]\n",
        "\n",
        "# выделение фич из самих маркеров\n",
        "markers_df = pd.read_csv(\"/content/markers_poly_filtered_sync.csv\").to_numpy()\n",
        "\n",
        "# делим данные на обучение/валидацию/тест\n",
        "test_percentage = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HTKGQ2YjzbF",
        "outputId": "ef4811a0-715e-47bb-c660-044b8a79437b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[139.  85.]\n",
            " [266.  95.]\n",
            " [252. 100.]\n",
            " ...\n",
            " [156.  95.]\n",
            " [171.  90.]\n",
            " [240.  nan]]\n",
            "[[139.  85.]\n",
            " [266.  95.]\n",
            " [252. 100.]\n",
            " ...\n",
            " [156.  95.]\n",
            " [171.  90.]\n",
            " [240.  89.]]\n"
          ]
        }
      ],
      "source": [
        "# импутирование данных\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "n_neighbors = 5\n",
        "\n",
        "labels = df_wheat[[\"Урожайность.зерна..г.\", \"Высота.растений..см\"]].to_numpy()[:n_plants_use]\n",
        "# labels = df_wheat[[\"Урожайность.зерна..г.\", \"Бурая.ржавчина...\"]].to_numpy()[:n_plants_use]\n",
        "# labels = df_wheat[[\"Урожайность.зерна..г.\", \"Желтая.ржавчина...\"]].to_numpy()[:n_plants_use]\n",
        "\n",
        "# (Пока просто средними значениями) импутируем данные, поскольку присутствуют пропуски\n",
        "# imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "imp = KNNImputer(n_neighbors=n_neighbors, weights='uniform')\n",
        "labels = imp.fit_transform(labels.reshape(-1, 2))\n",
        "\n",
        "# в случае с импутированием\n",
        "test_indices = np.random.choice(images.shape[0], int(images.shape[0] * test_percentage), replace=False)\n",
        "train_indices = np.setdiff1d(np.array(list(range(images.shape[0]))), test_indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "gcsx3cE8jzdE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a99472a-9b05-4331-f2de-67d9a2df8659"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[165  68  36  21   3 192  98 172  70  42 198 124  11   9  35 175  89  67\n",
            " 160 106]\n",
            "[  0   1   2   4   5   6   7   8  10  12  13  14  15  16  17  18  19  20\n",
            "  22  23  24  25  26  27  28  29  30  31  32  33  34  37  38  39  40  41\n",
            "  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60\n",
            "  61  62  63  64  65  66  69  71  72  73  74  75  76  77  78  79  80  81\n",
            "  82  83  84  85  86  87  88  90  91  92  93  94  95  96  97  99 100 101\n",
            " 102 103 104 105 107 108 109 110 111 112 113 114 115 116 117 118 119 120\n",
            " 121 122 123 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139\n",
            " 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157\n",
            " 158 159 161 162 163 164 166 167 168 169 170 171 173 174 176 177 178 179\n",
            " 180 181 182 183 184 185 186 187 188 189 190 191 193 194 195 196 197 199]\n"
          ]
        }
      ],
      "source": [
        "# отсеивание данных с пропусками\n",
        "df_wheat_no_nan = df_wheat[df_wheat[\"Урожайность.зерна..г.\"].notnull() & df_wheat[\"Высота.растений..см\"].notnull()]\n",
        "images = images[df_wheat_no_nan.index]\n",
        "total_features = total_features[df_wheat_no_nan.index]\n",
        "# df_wheat = df_wheat[df_wheat[\"Урожайность.зерна..г.\"].isnull() & df_wheat[\"Бурая.ржавчина...\"].isnull()]\n",
        "# df_wheat = df_wheat[df_wheat[\"Урожайность.зерна..г.\"].isnull() & df_wheat[\"Желтая.ржавчина...\"].isnull()]\n",
        "\n",
        "labels = df_wheat_no_nan[[\"Урожайность.зерна..г.\", \"Высота.растений..см\"]].to_numpy()[:n_plants_use]\n",
        "# labels = df_wheat[[\"Урожайность.зерна..г.\", \"Бурая.ржавчина...\"]].to_numpy()\n",
        "# labels = df_wheat[[\"Урожайность.зерна..г.\", \"Желтая.ржавчина...\"]].to_numpy()\n",
        "\n",
        "# в случае с фильтрованными данными\n",
        "test_indices = np.random.choice(np.array(list(range(labels.shape[0]))), size=int(labels.shape[0] * test_percentage), replace=False)\n",
        "train_indices = np.setdiff1d(np.array(list(range(labels.shape[0]))), test_indices)\n",
        "print(test_indices)\n",
        "print(train_indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "BIOIkL-XfzOH"
      },
      "outputs": [],
      "source": [
        "# нормализация данных при помощи логистической функции\n",
        "\n",
        "# разделяем фенотипы\n",
        "labels_1 = labels[:, 0]\n",
        "labels_2 = labels[:, 1]\n",
        "\n",
        "# нормализация, собственно\n",
        "labels_1 = rank_based_transform(labels[:, 0])\n",
        "labels_2 = rank_based_transform(labels[:, 1])\n",
        "\n",
        "labels = np.concatenate((labels_1.reshape(len(labels_1), 1),\n",
        "                         labels_2.reshape(len(labels_2), 1)), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VFxiJlwDf2_o"
      },
      "outputs": [],
      "source": [
        "# нормализация при помощи классической формулы - (значение - среднее) / дисперсия\n",
        "\n",
        "# разделяем фенотипы\n",
        "labels_1 = labels[:, 0]\n",
        "labels_2 = labels[:, 1]\n",
        "\n",
        "labels_1 = data_standardization(labels_1)\n",
        "labels_2 = data_standardization(labels_2)\n",
        "\n",
        "labels = np.concatenate((labels_1.reshape(len(labels_1), 1),\n",
        "                         labels_2.reshape(len(labels_2), 1)), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "CMs4G46dkmY0"
      },
      "outputs": [],
      "source": [
        "train_images, train_labels, train_dict = images[train_indices], labels[train_indices], total_features[train_indices]\n",
        "test_images, test_labels, test_dict = images[test_indices], labels[test_indices], total_features[test_indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "B2tXgRld2Sie"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "# Модель сугубо нейронной сети\n",
        "@dataclass\n",
        "class SimpleCNNModel:\n",
        "    n_epochs: int = 5\n",
        "    n_row: int = 200\n",
        "    n_col: int = 200\n",
        "    input_channels: int = 1\n",
        "    random_seed: int = 1234567890\n",
        "    n_dict_features: int = 30\n",
        "    n_trait: int = 2\n",
        "\n",
        "    data_train: np.ndarray = np.ndarray([])\n",
        "    features_train: np.ndarray = np.asarray([])\n",
        "    labels_train: np.ndarray = np.ndarray([])\n",
        "\n",
        "    data_test: np.ndarray = np.asarray([])\n",
        "    features_test: np.ndarray = np.asarray([])\n",
        "    labels_test: np.ndarray = np.asarray([])\n",
        "\n",
        "    def build(self, hp: dict):\n",
        "        \"\"\"\n",
        "        Функция построения модели нейросети с функциональным интерфейсом keras\n",
        "\n",
        "        :param hp: набор гиперпараметров, отвечающих за конфигурация нейросети\n",
        "        :return: граф-представление нейросети\n",
        "        \"\"\"\n",
        "\n",
        "        inp_node = Input((self.n_row, self.n_col, self.input_channels), name=\"img_input\")\n",
        "\n",
        "        inp_node_dict = Input({self.n_dict_features}, name=\"dict_input\")\n",
        "\n",
        "        conv_node_1 = Conv2D(hp['first_conv2d_out_channels'],\n",
        "                             kernel_size=(hp['first_conv2d_kernel_size'], hp['first_conv2d_kernel_size']),\n",
        "                             padding='same',\n",
        "                             strides=(1, 1),\n",
        "                             activation=hp['first_conv2d_activation'], name=\"conv_map_1\")(inp_node)\n",
        "        if hp['need_extra_conv2d']:\n",
        "            conv_node_1 = Conv2D(hp['extra_conv2d_out_channels'],\n",
        "                                 kernel_size=(hp['extra_conv2d_kernel_size'], hp['extra_conv2d_kernel_size']),\n",
        "                                 padding='same',\n",
        "                                 strides=(1, 1),\n",
        "                                 activation=hp['extra_conv2d_activation'], name=\"conv_map_extra\")(conv_node_1)\n",
        "\n",
        "        if hp['need_batch_norm_after_first_conv2d']:\n",
        "            batch_node_1 = BatchNormalization()(conv_node_1)\n",
        "            mp_node_1 = MaxPooling2D(pool_size=(2, 2))(batch_node_1)\n",
        "        else:\n",
        "            mp_node_1 = MaxPooling2D(pool_size=(2, 2))(conv_node_1)\n",
        "\n",
        "        conv_node_2 = Conv2D(hp['second_conv2d_out_channels'],\n",
        "                             kernel_size=(hp['second_conv2d_kernel_size'], hp['second_conv2d_kernel_size']),\n",
        "                             padding='same',\n",
        "                             strides=(1, 1),\n",
        "                             activation=hp['second_conv2d_activation'], name=\"conv_map_2\")(mp_node_1)\n",
        "\n",
        "        if hp['need_batch_norm_after_second_conv2d']:\n",
        "            batch_node_2 = BatchNormalization()(conv_node_2)\n",
        "            mp_node_2 = MaxPooling2D(pool_size=(2, 2), name=\"max_pool_map\")(batch_node_2)\n",
        "        else:\n",
        "            mp_node_2 = MaxPooling2D(pool_size=(2, 2), name=\"max_pool_map\")(conv_node_2)\n",
        "\n",
        "        if hp['need_deconv_block']:\n",
        "            deconv_node_2 = Conv2DTranspose(\n",
        "                hp['second_conv2d_out_channels'],\n",
        "                kernel_size=(hp['second_conv2d_kernel_size'], hp['second_conv2d_kernel_size']),\n",
        "                padding='same',\n",
        "                strides=(2, 2),\n",
        "                activation=hp['second_conv2d_activation'],\n",
        "                name=\"deconv_2\"\n",
        "            )(mp_node_2)\n",
        "            concat_node_2 = Concatenate(name=\"concat_2\", axis=3)([deconv_node_2, conv_node_2])\n",
        "            conv_node_deconv_2 = Conv2D(\n",
        "                hp['second_conv2d_out_channels'],\n",
        "                kernel_size=(hp['second_conv2d_kernel_size'], hp['second_conv2d_kernel_size']),\n",
        "                padding='same',\n",
        "                strides=(1, 1),\n",
        "                activation=hp['second_conv2d_activation'],\n",
        "                name=\"conv_deconv_2\"\n",
        "            )(concat_node_2)\n",
        "            deconv_node_1 = Conv2DTranspose(\n",
        "                hp['first_conv2d_out_channels'],\n",
        "                kernel_size=(hp['first_conv2d_kernel_size'], hp['first_conv2d_kernel_size']),\n",
        "                padding='same',\n",
        "                strides=(2, 2),\n",
        "                activation=hp['first_conv2d_activation'],\n",
        "                name=\"deconv_1\"\n",
        "            )(conv_node_deconv_2)\n",
        "            concat_node_1 = Concatenate(name=\"concat_1\", axis=3)([deconv_node_1, conv_node_1])\n",
        "            mp_node_2 = Conv2D(\n",
        "                hp['first_conv2d_out_channels'],\n",
        "                kernel_size=(hp['first_conv2d_kernel_size'], hp['first_conv2d_kernel_size']),\n",
        "                padding='same',\n",
        "                strides=(1, 1),\n",
        "                activation=hp['first_conv2d_activation'],\n",
        "                name=\"conv_deconv_1\"\n",
        "            )(concat_node_1)\n",
        "\n",
        "        if hp['use_gap_1_or_flatten_0'] == 0:\n",
        "            flatten_node = Flatten(name='flatten')(mp_node_2)\n",
        "            dense_node = Dense(hp['num_feature_output'], activation=hp['dense_output_activation'],\n",
        "                               name=\"img_feature_output\")(flatten_node)\n",
        "        elif hp['use_gap_1_or_flatten_0'] == 1:\n",
        "            dense_node = GlobalAveragePooling2D(name=\"img_feature_output\")(mp_node_2)\n",
        "\n",
        "        concatenate_features = Concatenate(name=\"concat_features\")([inp_node_dict, dense_node])\n",
        "\n",
        "        out = Dense(self.n_trait, activation='linear', name=\"cnn_multioutput\")(concatenate_features)\n",
        "\n",
        "        model = Model(inputs=[inp_node, inp_node_dict], outputs=out, name=\"regression_model\")\n",
        "\n",
        "        return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Z5t8EVBx1_ZP"
      },
      "outputs": [],
      "source": [
        "class ComboDataPool(keras.utils.Sequence):\n",
        "\n",
        "    def __init__(self, images, features, labels, batch_size: int, max_len: int = -1):\n",
        "        self.batch_size = batch_size\n",
        "        self.images = images[:max_len]\n",
        "        self.features = features[:max_len]\n",
        "        self.labels = labels[:max_len]\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(self.images.shape[0] / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_data = [self.images[idx * self.batch_size:(idx + 1) * self.batch_size],\n",
        "                   self.features[idx * self.batch_size:(idx + 1) * self.batch_size]]\n",
        "        batch_labels = self.labels[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "\n",
        "        return batch_data, batch_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "bjUvQz9x2OgG"
      },
      "outputs": [],
      "source": [
        "from itertools import product, islice\n",
        "from sklearn.model_selection import KFold\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "\n",
        "class ComboModelTuner:\n",
        "\n",
        "    @staticmethod\n",
        "    @tf.function\n",
        "    def fit_loss_mae(y_true: np.ndarray, y_pred: np.ndarray) -> np.array:\n",
        "        \"\"\"\n",
        "        MAE-функция потерь для обучения при помощи стандартного метода '.fit()'\n",
        "        \"\"\"\n",
        "        error = y_true - y_pred\n",
        "        abs_error_1, abs_error_2 = tf.abs(error[:, 0]), tf.abs(error[:, 1])\n",
        "        result_1, result_2 = tf.reduce_mean(abs_error_1), tf.reduce_mean(abs_error_2)\n",
        "        return (result_1 + result_2) / 2\n",
        "\n",
        "    @staticmethod\n",
        "    @tf.function\n",
        "    def fit_loss_mse(y_true: np.ndarray, y_pred: np.ndarray) -> np.array:\n",
        "        \"\"\"\n",
        "        MSE-функция потерь для обучения при помощи стандартного метода '.fit()'\n",
        "        \"\"\"\n",
        "        error = y_true - y_pred\n",
        "        squared_error_1, squared_error_2 = tf.square(error), tf.square(error[:, 1])\n",
        "        result_1, result_2 = tf.reduce_mean(squared_error_1), tf.reduce_mean(squared_error_2)\n",
        "        return (result_1 + result_2) / 2\n",
        "\n",
        "    @staticmethod\n",
        "    @tf.function\n",
        "    def custom_loss_mae(y_true: np.ndarray, y_pred: np.ndarray) -> np.array:\n",
        "        \"\"\"\n",
        "        MAE-функция потерь для обучения при помощи пользовательской реализации цикла обучения\n",
        "        \"\"\"\n",
        "        error = y_true - y_pred\n",
        "        abs_error_1, abs_error_2 = tf.abs(error[:, 0]), tf.abs(error[:, 1])\n",
        "        result_1, result_2 = tf.reduce_mean(abs_error_1), tf.reduce_mean(abs_error_2)\n",
        "        return (result_1 + result_2) / 2\n",
        "\n",
        "    @staticmethod\n",
        "    @tf.function\n",
        "    def custom_loss_mse(y_true: np.ndarray, y_pred: np.ndarray) -> np.array:\n",
        "        \"\"\"\n",
        "        MSE-функция потерь для обучения при помощи пользовательской реализации цикла обучения\n",
        "        \"\"\"\n",
        "        error = y_true - y_pred\n",
        "        squared_error_1, squared_error_2 = tf.square(error), tf.square(error[:, 1])\n",
        "        result_1, result_2 = tf.reduce_mean(squared_error_1), tf.reduce_mean(squared_error_2)\n",
        "        return (result_1 + result_2) / 2\n",
        "\n",
        "    @staticmethod\n",
        "    def custom_cv(total_hp: dict, cnn_hp: dict, model: 'SimpleCNNModel',\n",
        "                  splits_num: int = 10,\n",
        "                  early_stop: bool = True, model_checkpoint: bool = True,\n",
        "                  data_generator: bool = False) -> list:\n",
        "        model_keras = model.build(cnn_hp)\n",
        "\n",
        "        learning_data_pool = ComboDataPool(images=train_images,\n",
        "                                           features=train_dict,\n",
        "                                           labels=train_labels,\n",
        "                                           batch_size=64)\n",
        "\n",
        "        callbacks = []\n",
        "        if early_stop:\n",
        "            callback_early_stop = EarlyStopping(monitor=\"loss\", min_delta=0.001, patience=2, verbose=1)\n",
        "            callbacks.append(callback_early_stop)\n",
        "        if model_checkpoint:\n",
        "            callback_checkpoint = ModelCheckpoint(filepath=\"checkpoints/model_no_df_{epoch:02d}-{val_loss:.2f}.keras\",\n",
        "                                                  save_best_only=True, monitor=\"loss\", verbose=1)\n",
        "            callbacks.append(callback_checkpoint)\n",
        "\n",
        "        model_keras.compile(optimizer=keras.optimizers.SGD(learning_rate=0.0001),\n",
        "                            loss=ComboModelTuner.fit_loss_mae,\n",
        "                            metrics=[ComboModelTuner.fit_loss_mse])\n",
        "\n",
        "        learning_data_pool = ComboDataPool(images=train_images,\n",
        "                                           features=train_dict,\n",
        "                                           labels=train_labels,\n",
        "                                           batch_size=64)\n",
        "\n",
        "        mae_per_fold_tr, mse_per_fold_tr = [], []\n",
        "        mae_per_fold_vd, mse_per_fold_vd = [], []\n",
        "\n",
        "        kfold = KFold(n_splits=splits_num, shuffle=True)\n",
        "        for j, (tr_idx, val_idx) in enumerate(kfold.split(model.features_train, model.data_train, model.labels_train)):\n",
        "            if data_generator:\n",
        "                history = model_keras.fit(learning_data_pool, epochs=total_hp[\"num_epochs_ll\"],\n",
        "                                      validation_data=([model.data_train[val_idx], model.features_train[val_idx]],\n",
        "                                                      model.labels_train[val_idx]))\n",
        "            else:\n",
        "                history = model_keras.fit(x=[model.data_train[tr_idx], model.features_train[tr_idx]],\n",
        "                                          y=model.labels_train[tr_idx],\n",
        "                                          batch_size=total_hp[\"batch_size_ll\"],\n",
        "                                          epochs=total_hp[\"num_epochs_ll\"],\n",
        "                                          validation_data=([model.data_train[val_idx], model.features_train[val_idx]],\n",
        "                                                          model.labels_train[val_idx]),\n",
        "                                          callbacks=callbacks)\n",
        "\n",
        "            scores = model_keras.evaluate(x=[model.data_train[tr_idx], model.features_train[tr_idx]],\n",
        "                                          y=model.labels_train[tr_idx])\n",
        "\n",
        "            mse_per_fold_tr.append(scores[0])\n",
        "            mae_per_fold_tr.append(scores[1])\n",
        "\n",
        "            scores = model_keras.evaluate(x=[model.data_train[val_idx], model.features_train[val_idx]],\n",
        "                                          y=model.labels_train[val_idx])\n",
        "\n",
        "            mse_per_fold_vd.append(scores[0])\n",
        "            mae_per_fold_vd.append(scores[1])\n",
        "            print(f\"Fold #{j + 1} finished succesfully\")\n",
        "\n",
        "        return [mae_per_fold_tr, mse_per_fold_tr, mae_per_fold_vd, mse_per_fold_vd], model_keras\n",
        "\n",
        "    @staticmethod\n",
        "    def fit_cv(total_hp: dict, cnn_hp: dict, model: 'SimpleCNNModel',\n",
        "               splits_num: int = 10,\n",
        "               early_stop: bool = True, model_checkpoint: bool = True,\n",
        "               data_generator: bool = False) -> list:\n",
        "        model_keras = model.build(cnn_hp)\n",
        "\n",
        "        learning_data_pool = ComboDataPool(images=train_images,\n",
        "                                           features=train_dict,\n",
        "                                           labels=train_labels,\n",
        "                                           batch_size=64)\n",
        "\n",
        "        callbacks = []\n",
        "        if early_stop:\n",
        "            callback_early_stop = EarlyStopping(monitor=\"loss\", min_delta=0.001, patience=2, verbose=1)\n",
        "            callbacks.append(callback_early_stop)\n",
        "        if model_checkpoint:\n",
        "            callback_checkpoint = ModelCheckpoint(filepath=\"checkpoints/model_no_df_{epoch:02d}-{val_loss:.2f}.keras\",\n",
        "                                                  save_best_only=True, monitor=\"loss\", verbose=1)\n",
        "            callbacks.append(callback_checkpoint)\n",
        "\n",
        "        model_keras.compile(optimizer=keras.optimizers.SGD(learning_rate=0.0001),\n",
        "                            loss=ComboModelTuner.custom_loss_mae,\n",
        "                            metrics=[ComboModelTuner.custom_loss_mse])\n",
        "\n",
        "        mae_per_fold_tr, mse_per_fold_tr = [], []\n",
        "        mae_per_fold_vd, mse_per_fold_vd = [], []\n",
        "\n",
        "        mae_per_fold_tr_epochend, mse_per_fold_tr_epochend = [], []\n",
        "        mae_per_fold_vd_epochend, mse_per_fold_vd_epochend = [], []\n",
        "\n",
        "        kfold = KFold(n_splits=splits_num, shuffle=True)\n",
        "        for j, (tr_idx, val_idx) in enumerate(kfold.split(model.features_train, model.data_train, model.labels_train)):\n",
        "            if data_generator:\n",
        "                history = model_keras.fit(learning_data_pool, epochs=total_hp[\"num_epochs_ll\"],\n",
        "                                          validation_data=([model.data_train[val_idx], model.features_train[val_idx]],\n",
        "                                                          model.labels_train[val_idx]))\n",
        "            else:\n",
        "                history = model_keras.fit(x=[model.data_train[tr_idx], model.features_train[tr_idx]],\n",
        "                                          y=model.labels_train[tr_idx],\n",
        "                                          batch_size=total_hp[\"batch_size_ll\"],\n",
        "                                          epochs=total_hp[\"num_epochs_ll\"],\n",
        "                                          validation_data=([model.data_train[val_idx], model.features_train[val_idx]],\n",
        "                                                          model.labels_train[val_idx]),\n",
        "                                          callbacks=callbacks)\n",
        "\n",
        "            scores = model_keras.evaluate(x=[model.data_train[tr_idx], model.features_train[tr_idx]],\n",
        "                                          y=model.labels_train[tr_idx])\n",
        "\n",
        "            mae_per_fold_tr_epochend.append(scores[0])\n",
        "            mse_per_fold_tr_epochend.append(scores[1])\n",
        "\n",
        "            scores = model_keras.evaluate(x=[model.data_train[val_idx], model.features_train[val_idx]],\n",
        "                                          y=model.labels_train[val_idx])\n",
        "\n",
        "            mae_per_fold_vd_epochend.append(scores[0])\n",
        "            mse_per_fold_vd_epochend.append(scores[1])\n",
        "\n",
        "            tmp_mae_tr = history.history[\"loss\"]\n",
        "            tmp_mse_tr = history.history[\"custol_loss_mse\"]\n",
        "            tmp_mae_val = history.history[\"val_loss\"]\n",
        "            tmp_mse_val = history.history[\"val_custom_loss_mse\"]\n",
        "\n",
        "            mae_per_fold_tr = mse_per_fold_tr + tmp_mae_tr\n",
        "            mse_per_fold_tr = mae_per_fold_tr + tmp_mae_tr\n",
        "            mae_per_fold_vd = mae_per_fold_vd + tmp_mae_val\n",
        "            mse_per_fold_vd = mse_per_fold_vd + tmp_mse_val\n",
        "            print(f\"Fold #{j + 1} finished succesfully\")\n",
        "\n",
        "        return [[mae_per_fold_tr_epochend, mse_per_fold_tr_epochend, mae_per_fold_vd_epochend, mse_per_fold_vd_epochend],\n",
        "                [mae_per_fold_tr, mse_per_fold_tr, mae_per_fold_vd, mse_per_fold_vd]], model_keras\n",
        "\n",
        "    @staticmethod\n",
        "    def random_hyper_tuning(iters_num: int, hps_cnn: dict,\n",
        "                            train_images_: np.ndarray, train_features_: np.ndarray, train_labels_: np.ndarray,\n",
        "                            test_images_: np.ndarray, test_features_: np.ndarray, test_labels_: np.ndarray,\n",
        "                            early_stop: bool = True, model_checkpoint: bool = False,\n",
        "                            save_folder_name: str = \"gg\"):\n",
        "\n",
        "        test_mae, test_mse = [], []\n",
        "\n",
        "        fl_mse_train_epochend = open(f\"metrics/{save_folder_name}/mse_trains_{save_folder_name}_epochend.pickle\", \"wb\")\n",
        "        fl_mae_train_epochend = open(f\"metrics/{save_folder_name}/mae_trains_{save_folder_name}_epochend.pickle\", \"wb\")\n",
        "        fl_mse_valid_epochend = open(f\"metrics/{save_folder_name}/mse_valid_{save_folder_name}_epochend.pickle\", \"wb\")\n",
        "        fl_mae_valid_epochend = open(f\"metrics/{save_folder_name}/mae_valid_{save_folder_name}_epochend.pickle\", \"wb\")\n",
        "\n",
        "        fl_mse_train = open(f\"metrics/{save_folder_name}/mse_trains_{save_folder_name}.pickle\", \"wb\")\n",
        "        fl_mae_train = open(f\"metrics/{save_folder_name}/mae_trains_{save_folder_name}.pickle\", \"wb\")\n",
        "        fl_mse_valid = open(f\"metrics/{save_folder_name}/mse_valid_{save_folder_name}.pickle\", \"wb\")\n",
        "        fl_mae_valid = open(f\"metrics/{save_folder_name}/mae_valid_{save_folder_name}.pickle\", \"wb\")\n",
        "\n",
        "        for iter_ in range(iters_num):\n",
        "            # Сборка комбинации случайных гиперпараметров в заданын границах\n",
        "            print(f\"Random Tuning iter #{iter_} started\")\n",
        "\n",
        "            cnn_hp_comb = {}\n",
        "\n",
        "            for param in hps_cnn:\n",
        "                if len(hps_cnn[param]) > 1:\n",
        "                    if any(isinstance(x, bool) for x in hps_cnn[param]) or \\\n",
        "                            any(isinstance(x, str) for x in hps_cnn[param]):\n",
        "                        cnn_hp_comb[param] = hps_cnn[param][np.random.randint(len(hps_cnn[param]))]\n",
        "                    else:\n",
        "                        cnn_hp_comb[param] = np.random.randint(low=min(hps_cnn[param]), high=max(hps_cnn[param]))\n",
        "                else:\n",
        "                    cnn_hp_comb[param] = hps_cnn[param][0]\n",
        "\n",
        "            print(cnn_hp_comb)\n",
        "\n",
        "            num_epochs = 5\n",
        "\n",
        "            model = SimpleCNNModel(n_epochs=num_epochs,\n",
        "                                   n_row=200,\n",
        "                                   n_col=200,\n",
        "                                   input_channels=1,\n",
        "                                   random_seed=1234567890,\n",
        "                                   n_dict_features=7,\n",
        "                                   n_trait=2,\n",
        "                                   data_train=train_images_,\n",
        "                                   labels_train=train_labels_,\n",
        "                                   features_train=train_features_,\n",
        "                                   data_test=test_images_,\n",
        "                                   features_test=test_features_,\n",
        "                                   labels_test=test_labels_)\n",
        "\n",
        "            metrics, model = ComboModelTuner.fit_cv(total_hp={\"batch_size_ll\": 64, \"num_epochs_ll\": num_epochs},\n",
        "                                                    cnn_hp=cnn_hp_comb,\n",
        "                                                    splits_num=5,\n",
        "                                                    model=model,\n",
        "                                                    early_stop=early_stop,\n",
        "                                                    model_checkpoint=model_checkpoint)\n",
        "\n",
        "            model.save(f\"/content/model_saves/{save_folder_name}/rand_cv_trained_model_iter{iter_}.keras\")\n",
        "\n",
        "            pickle.dump(metrics[0], fl_mae_train_epochend)\n",
        "            pickle.dump(metrics[1], fl_mse_train_epochend)\n",
        "            pickle.dump(metrics[2], fl_mae_valid_epochend)\n",
        "            pickle.dump(metrics[3], fl_mse_valid_epochend)\n",
        "\n",
        "            pickle.dump(metrics[4], fl_mae_train)\n",
        "            pickle.dump(metrics[5], fl_mse_train)\n",
        "            pickle.dump(metrics[6], fl_mae_valid)\n",
        "            pickle.dump(metrics[7], fl_mse_valid)\n",
        "\n",
        "            # считаем ошибку модели на тестовой выборке\n",
        "            scores_test = model.predict([test_images_, test_features_], test_labels_)\n",
        "\n",
        "            test_mae.append(scores_test[0])\n",
        "            test_mse.append(scores_test[1])\n",
        "\n",
        "            print(f\"Random Tuning iter #{iter_} finished successfully\")\n",
        "\n",
        "        with open(f\"metrics/{save_folder_name}/mae_test_{save_folder_name}.pickle\", \"wb\") as fl:\n",
        "            pickle.dump(test_mae, fl)\n",
        "        with open(f\"metrics/{save_folder_name}/mse_test_{save_folder_name}.pickle\", \"wb\") as fl:\n",
        "            pickle.dump(test_mse, fl)\n",
        "\n",
        "    @staticmethod\n",
        "    def grid_hyper_tuning(iters_num: int, hps_cnn: dict,\n",
        "                          train_images_: np.ndarray, train_features_: np.ndarray, train_labels_: np.ndarray,\n",
        "                          test_images_: np.ndarray, test_features_: np.ndarray, test_labels_: np.ndarray,\n",
        "                          early_stop: bool = True, model_checkpoint: bool = False,\n",
        "                          save_folder_name: str = \"gg\"):\n",
        "        cnn_hp_combos = (dict(zip(hps_cnn.keys(), values)) for values in product(*hps_cnn.values()))\n",
        "\n",
        "        test_mae, test_mse = [], []\n",
        "\n",
        "        fl_mse_train_epochend = open(f\"metrics/{save_folder_name}/mse_trains_{save_folder_name}_epochend.pickle\", \"wb\")\n",
        "        fl_mae_train_epochend = open(f\"metrics/{save_folder_name}/mae_trains_{save_folder_name}_epochend.pickle\", \"wb\")\n",
        "        fl_mse_valid_epochend = open(f\"metrics/{save_folder_name}/mse_valid_{save_folder_name}_epochend.pickle\", \"wb\")\n",
        "        fl_mae_valid_epochend = open(f\"metrics/{save_folder_name}/mae_valid_{save_folder_name}_epochend.pickle\", \"wb\")\n",
        "\n",
        "        fl_mse_train = open(f\"metrics/{save_folder_name}/mse_trains_{save_folder_name}.pickle\", \"wb\")\n",
        "        fl_mae_train = open(f\"metrics/{save_folder_name}/mae_trains_{save_folder_name}.pickle\", \"wb\")\n",
        "        fl_mse_valid = open(f\"metrics/{save_folder_name}/mse_valid_{save_folder_name}.pickle\", \"wb\")\n",
        "        fl_mae_valid = open(f\"metrics/{save_folder_name}/mae_valid_{save_folder_name}.pickle\", \"wb\")\n",
        "\n",
        "        for i, tmp_hps_cnn in enumerate(islice(cnn_hp_combos, iters_num)):\n",
        "\n",
        "            num_epochs = 5\n",
        "\n",
        "            model = SimpleCNNModel(n_epochs=num_epochs,\n",
        "                                   n_row=200,\n",
        "                                   n_col=200,\n",
        "                                   input_channels=1,\n",
        "                                   random_seed=1234567890,\n",
        "                                   n_dict_features=7,\n",
        "                                   n_trait=2,\n",
        "                                   data_train=train_images_,\n",
        "                                   labels_train=train_labels_,\n",
        "                                   features_train=train_features_,\n",
        "                                   data_test=test_images_,\n",
        "                                   features_test=test_features_,\n",
        "                                   labels_test=test_labels_)\n",
        "\n",
        "            metrics, model = ComboModelTuner.fit_cv(total_hp={\"batch_size_ll\": 64, \"num_epochs_ll\": num_epochs},\n",
        "                                                    cnn_hp=tmp_hps_cnn,\n",
        "                                                    splits_num=5,\n",
        "                                                    model=model,\n",
        "                                                    early_stop=early_stop,\n",
        "                                                    model_checkpoint=model_checkpoint)\n",
        "\n",
        "            pickle.dump(metrics[0], fl_mae_train_epochend)\n",
        "            pickle.dump(metrics[1], fl_mse_train_epochend)\n",
        "            pickle.dump(metrics[2], fl_mae_valid_epochend)\n",
        "            pickle.dump(metrics[3], fl_mse_valid_epochend)\n",
        "\n",
        "            pickle.dump(metrics[4], fl_mae_train)\n",
        "            pickle.dump(metrics[5], fl_mse_train)\n",
        "            pickle.dump(metrics[6], fl_mae_valid)\n",
        "            pickle.dump(metrics[7], fl_mse_valid)\n",
        "\n",
        "            model.save(f\"/content/model_saves/{save_folder_name}/grid_cv_trained_model_iter{i}.keras\")\n",
        "\n",
        "            # считаем ошибку модели на тестовой выборке\n",
        "            scores_test = model.evaluate([test_images_, test_features_], test_labels_)\n",
        "\n",
        "            test_mae.append(scores_test[0])\n",
        "            test_mse.append(scores_test[1])\n",
        "            print(f\"Grid Tuning iter #{i + 1} finished successfully\")\n",
        "\n",
        "        with open(f\"metrics/{save_folder_name}/mae_test_{save_folder_name}.pickle\", \"wb\") as fl:\n",
        "            pickle.dump(test_mae, fl)\n",
        "        with open(f\"metrics/{save_folder_name}/mse_test_{save_folder_name}.pickle\", \"wb\") as fl:\n",
        "            pickle.dump(test_mse, fl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "FzfGemud2HsX",
        "outputId": "cfb6215b-6d9b-44e4-e757-9ef7960fc6db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-bfa9c8eaab95>\u001b[0m in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m#                                     save_folder_name=\"height_crop_test\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m ComboModelTuner.grid_hyper_tuning(20, model_hp,\n\u001b[0m\u001b[1;32m     33\u001b[0m                                   \u001b[0mtrain_images_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                                   \u001b[0mtrain_labels_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-c3fb0eb80463>\u001b[0m in \u001b[0;36mgrid_hyper_tuning\u001b[0;34m(iters_num, hps_cnn, train_images_, train_features_, train_labels_, test_images_, test_features_, test_labels_, early_stop, model_checkpoint, save_folder_name)\u001b[0m\n\u001b[1;32m    273\u001b[0m                                    labels_test=test_labels_)\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m             metrics, model = ComboModelTuner.fit_cv(total_hp={\"batch_size_ll\": 64, \"num_epochs_ll\": num_epochs},\n\u001b[0m\u001b[1;32m    276\u001b[0m                                                     \u001b[0mcnn_hp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtmp_hps_cnn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m                                                     \u001b[0msplits_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-c3fb0eb80463>\u001b[0m in \u001b[0;36mfit_cv\u001b[0;34m(total_hp, cnn_hp, model, splits_num, early_stop, model_checkpoint, data_generator)\u001b[0m\n\u001b[1;32m    151\u001b[0m                                                           model.labels_train[val_idx]))\n\u001b[1;32m    152\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m                 history = model_keras.fit(x=[model.data_train[tr_idx], model.features_train[tr_idx]],\n\u001b[0m\u001b[1;32m    154\u001b[0m                                           \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtr_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m                                           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_hp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"batch_size_ll\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1805\u001b[0m                         ):\n\u001b[1;32m   1806\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1807\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1808\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    903\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m         \u001b[0;31m# no_variable_creation function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 905\u001b[0;31m         return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    906\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1322\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1487\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# задаем сетку гиперпараметров для\n",
        "import pickle\n",
        "\n",
        "model_hp = {# сначала идут параметры сверточной части модели\n",
        "            'first_conv2d_out_channels': [32, 64],\n",
        "            'first_conv2d_kernel_size': [3, 5, 7],\n",
        "            'first_conv2d_activation': ['tanh', 'relu'],\n",
        "            'need_extra_conv2d': [False, True],\n",
        "            'extra_conv2d_out_channels': [32, 64],\n",
        "            'extra_conv2d_kernel_size': [3, 5, 7],\n",
        "            'extra_conv2d_activation': ['tanh', 'relu'],\n",
        "            'need_batch_norm_after_first_conv2d': [True, False],\n",
        "            'second_conv2d_kernel_size': [3, 5],\n",
        "            'second_conv2d_out_channels': [64, 128],\n",
        "            'second_conv2d_activation': ['tanh', 'relu'],\n",
        "            'need_batch_norm_after_second_conv2d': [True, False],\n",
        "            'dense_output_activation': ['sigmoid', 'linear'],\n",
        "            'use_gap_1_or_flatten_0': [1, 0],\n",
        "            'need_deconv_block': [False, True],\n",
        "            'num_feature_output': [64, 128, 256],\n",
        "        }\n",
        "\n",
        "# ComboModelTuner.random_hyper_tuning(10, model_hp,\n",
        "#                                     train_images_=train_images,\n",
        "#                                     train_labels_=train_labels,\n",
        "#                                     train_features_=train_dict,\n",
        "#                                     test_images_=test_images,\n",
        "#                                     test_labels_=test_labels,\n",
        "#                                     test_features_=test_dict,\n",
        "#                                     save_folder_name=\"height_crop_test\")\n",
        "\n",
        "ComboModelTuner.grid_hyper_tuning(20, model_hp,\n",
        "                                  train_images_=train_images,\n",
        "                                  train_labels_=train_labels,\n",
        "                                  train_features_=train_dict,\n",
        "                                  test_images_=test_images,\n",
        "                                  test_labels_=test_labels,\n",
        "                                  test_features_=test_dict,\n",
        "                                  early_stop=True,\n",
        "                                  model_checkpoint=False,\n",
        "                                  save_folder_name=\"height_crop_test\")\n",
        "\n",
        "# сохраним индексы разбиения на обучающий и тестовый наборы данных. Сначала идет\n",
        "# обучающий набор (train), а затем тестовый (test)\n",
        "with open(\"/content/indices/train_test_split.txt\", \"rb\") as fl:\n",
        "    pickle.dump(train_indices, fl)\n",
        "    pickle.dump(test_indices, fl)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# архивирование и загрузка истории обучения модели + индексирования образцов на локальную машину\n",
        "!cd /contents\n",
        "!zip -r model_checkpoints.zip model_saves/\n",
        "!zip -r metrics.zip metrics/\n",
        "!zip -r train_test_indices.zip indices/\n",
        "\n",
        "from google.colab import files\n",
        "files.download('model_checkpoints.zip')\n",
        "files.download('train_test_indices.zip')\n",
        "files.download('metrics.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "kN5PrXjsoTx8",
        "outputId": "03f3b787-a2a2-482d-8fb1-875261cedbe9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: cd: /contents: No such file or directory\n",
            "\tzip warning: name not matched: model_saves/\n",
            "\n",
            "zip error: Nothing to do! (try: zip -r model_checkpoints.zip . -i model_saves/)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "Cannot find file: model_checkpoints.zip",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-603a9a369870>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'zip -r model_checkpoints.zip model_saves/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_checkpoints.zip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    223\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Cannot find file: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=undefined-variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m   \u001b[0mcomm_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_IPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomm_manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Cannot find file: model_checkpoints.zip"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}