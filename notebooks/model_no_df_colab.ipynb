{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqeQ0cHQnCbv",
        "outputId": "2dd99901-a069-4d99-de72-7e1b5cfa5d09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting umap-learn\n",
            "  Downloading umap_learn-0.5.6-py3-none-any.whl (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.2.2)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (0.58.1)\n",
            "Collecting pynndescent>=0.5 (from umap-learn)\n",
            "  Downloading pynndescent-0.5.12-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from umap-learn) (4.66.4)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.2->umap-learn) (0.41.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from pynndescent>=0.5->umap-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22->umap-learn) (3.5.0)\n",
            "Installing collected packages: pynndescent, umap-learn\n",
            "Successfully installed pynndescent-0.5.12 umap-learn-0.5.6\n"
          ]
        }
      ],
      "source": [
        "!pip install umap-learn\n",
        "# !pip install numpy\n",
        "# !pip install pandas\n",
        "# !pip install keras\n",
        "# !pip install scikit-learn\n",
        "# !pip install opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DF36u6uEwBMB"
      },
      "outputs": [],
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import Sequential, Model\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers import Concatenate\n",
        "from keras.layers import GlobalAveragePooling2D\n",
        "from keras.layers import Conv2DTranspose\n",
        "from keras.layers import Resizing\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import numpy as np\n",
        "from scipy.stats import rankdata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rE7GAxmts9cC"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.impute import KNNImputer\n",
        "import umap\n",
        "\n",
        "def pca_features(data: np.ndarray, n_components: int = 10) -> np.ndarray:\n",
        "    flattened_data = np.array([img.flatten() for img in data])\n",
        "    data_processed = PCA(n_components=n_components).fit_transform(flattened_data)\n",
        "    return data_processed\n",
        "\n",
        "\n",
        "def t_sne_features(data: np.ndarray, n_components: int = 10):\n",
        "    flattened_data = np.array([img.flatten() for img in data])\n",
        "    data_embeded = TSNE(n_components=n_components,\n",
        "                        learning_rate='auto',\n",
        "                        init='random',\n",
        "                        method='exact',\n",
        "                        perplexity=3).fit_transform(flattened_data)\n",
        "    return data_embeded\n",
        "\n",
        "def umap_features(data: np.ndarray, n_components: int = 10):\n",
        "    flattened_data = np.array([img.flatten() for img in data])\n",
        "    data_processed = umap.UMAP().fit_transform(flattened_data)\n",
        "    return data_processed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9uUnR0C8ZI3"
      },
      "outputs": [],
      "source": [
        "def rank_based_transform(x, k=0.5):\n",
        "    num_samp = np.sum(~np.isnan(x))\n",
        "    ranks = (rankdata(x, method='ordinal').astype(float) - k) / (num_samp - 2 * k + 1)\n",
        "    return np.log(ranks / (1 - ranks))\n",
        "\n",
        "\n",
        "def data_standardization(arr: np.ndarray) -> np.ndarray:\n",
        "    return (arr - arr.mean()) / arr.std()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EAlpGeePtNK0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2 as cv\n",
        "import pandas as pd\n",
        "\n",
        "def load_images_from_folder(folder: str) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Функция подгрузки необходимого набора искусственных изображений из передаваемого каталога.\n",
        "\n",
        "    :param folder: папка с изображениями, сохраненными в формате .png\n",
        "\n",
        "    :return: список формата Numpy, содержащие AIO в объектах класса Image из Pillow\n",
        "    \"\"\"\n",
        "\n",
        "    images = []\n",
        "    for filename in os.listdir(folder):\n",
        "        img = cv.imread(os.path.join(folder, filename), cv.IMREAD_GRAYSCALE)\n",
        "        if img is not None:\n",
        "            images.append(np.asarray(img).astype(np.float32))\n",
        "    return np.asarray(images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ShEu8HAhfY1z"
      },
      "outputs": [],
      "source": [
        "n_pca, n_t_sne, n_umap = 5, 2, 5\n",
        "n_plants_use = 200\n",
        "\n",
        "folder_images = \"/content\"\n",
        "images = load_images_from_folder(folder_images)\n",
        "\n",
        "# выделение фич из изображений\n",
        "pca_features_ = pca_features(images, n_components=n_pca)\n",
        "t_sne_features_ = t_sne_features(images, n_components=n_t_sne)\n",
        "# umap_features_ = umap_features(images, n_components=n_umap)\n",
        "\n",
        "total_features = np.concatenate((pca_features_, t_sne_features_), axis=1)[:n_plants_use]\n",
        "\n",
        "df_wheat = pd.read_csv(\"/content/wheat_pheno_num_sync.csv\")[:n_plants_use]\n",
        "\n",
        "# выделение фич из самих маркеров\n",
        "markers_df = pd.read_csv(\"/content/markers_poly_filtered_sync.csv\").to_numpy()\n",
        "\n",
        "# делим данные на обучение/валидацию/тест\n",
        "test_percentage = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HTKGQ2YjzbF",
        "outputId": "ef4811a0-715e-47bb-c660-044b8a79437b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[139.  85.]\n",
            " [266.  95.]\n",
            " [252. 100.]\n",
            " ...\n",
            " [156.  95.]\n",
            " [171.  90.]\n",
            " [240.  nan]]\n",
            "[[139.  85.]\n",
            " [266.  95.]\n",
            " [252. 100.]\n",
            " ...\n",
            " [156.  95.]\n",
            " [171.  90.]\n",
            " [240.  89.]]\n"
          ]
        }
      ],
      "source": [
        "# импутирование данных\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "n_neighbors = 5\n",
        "\n",
        "labels = df_wheat[[\"Урожайность.зерна..г.\", \"Высота.растений..см\"]].to_numpy()[:n_plants_use]\n",
        "# labels = df_wheat[[\"Урожайность.зерна..г.\", \"Бурая.ржавчина...\"]].to_numpy()[:n_plants_use]\n",
        "# labels = df_wheat[[\"Урожайность.зерна..г.\", \"Желтая.ржавчина...\"]].to_numpy()[:n_plants_use]\n",
        "\n",
        "# (Пока просто средними значениями) импутируем данные, поскольку присутствуют пропуски\n",
        "# imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "imp = KNNImputer(n_neighbors=n_neighbors, weights='uniform')\n",
        "labels = imp.fit_transform(labels.reshape(-1, 2))\n",
        "\n",
        "# в случае с импутированием\n",
        "test_indices = np.random.choice(images.shape[0], int(images.shape[0] * test_percentage), replace=False)\n",
        "train_indices = np.setdiff1d(np.array(list(range(images.shape[0]))), test_indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcsx3cE8jzdE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecef5858-f202-4c1d-a4d9-b428a405edd2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  1 139   9  11  28  69  37  66   6 140  94 109  30 158   5  34  48 124\n",
            " 199 131]\n",
            "[  0   2   3   4   7   8  10  12  13  14  15  16  17  18  19  20  21  22\n",
            "  23  24  25  26  27  29  31  32  33  35  36  38  39  40  41  42  43  44\n",
            "  45  46  47  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63\n",
            "  64  65  67  68  70  71  72  73  74  75  76  77  78  79  80  81  82  83\n",
            "  84  85  86  87  88  89  90  91  92  93  95  96  97  98  99 100 101 102\n",
            " 103 104 105 106 107 108 110 111 112 113 114 115 116 117 118 119 120 121\n",
            " 122 123 125 126 127 128 129 130 132 133 134 135 136 137 138 141 142 143\n",
            " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 159 160 161 162\n",
            " 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180\n",
            " 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198]\n"
          ]
        }
      ],
      "source": [
        "# отсеивание данных с пропусками\n",
        "# df_wheat_no_nan = df_wheat[df_wheat[\"Урожайность.зерна..г.\"].notnull() & df_wheat[\"Высота.растений..см\"].notnull()]\n",
        "# df_wheat_no_nan = df_wheat[df_wheat[\"Урожайность.зерна..г.\"].notnull() & df_wheat[\"Бурая.ржавчина...\"].notnull()]\n",
        "df_wheat_no_nan = df_wheat[df_wheat[\"Урожайность.зерна..г.\"].notnull() & df_wheat[\"Желтая.ржавчина...\"].notnull()]\n",
        "images = images[df_wheat_no_nan.index]\n",
        "total_features = total_features[df_wheat_no_nan.index]\n",
        "\n",
        "# labels = df_wheat_no_nan[[\"Урожайность.зерна..г.\", \"Высота.растений..см\"]].to_numpy()[:n_plants_use]\n",
        "# labels = df_wheat[[\"Урожайность.зерна..г.\", \"Бурая.ржавчина...\"]].to_numpy()\n",
        "labels = df_wheat[[\"Урожайность.зерна..г.\", \"Желтая.ржавчина...\"]].to_numpy()\n",
        "\n",
        "# в случае с фильтрованными данными\n",
        "test_indices = np.random.choice(np.array(list(range(labels.shape[0]))), size=int(labels.shape[0] * test_percentage), replace=False)\n",
        "train_indices = np.setdiff1d(np.array(list(range(labels.shape[0]))), test_indices)\n",
        "print(test_indices)\n",
        "print(train_indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIOIkL-XfzOH"
      },
      "outputs": [],
      "source": [
        "# нормализация данных при помощи логистической функции\n",
        "\n",
        "# разделяем фенотипы\n",
        "labels_1 = labels[:, 0]\n",
        "labels_2 = labels[:, 1]\n",
        "\n",
        "# нормализация, собственно\n",
        "labels_1 = rank_based_transform(labels[:, 0])\n",
        "labels_2 = rank_based_transform(labels[:, 1])\n",
        "\n",
        "labels = np.concatenate((labels_1.reshape(len(labels_1), 1),\n",
        "                         labels_2.reshape(len(labels_2), 1)), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VFxiJlwDf2_o"
      },
      "outputs": [],
      "source": [
        "# нормализация при помощи классической формулы - (значение - среднее) / дисперсия\n",
        "\n",
        "# разделяем фенотипы\n",
        "labels_1 = labels[:, 0]\n",
        "labels_2 = labels[:, 1]\n",
        "\n",
        "labels_1 = data_standardization(labels_1)\n",
        "labels_2 = data_standardization(labels_2)\n",
        "\n",
        "labels = np.concatenate((labels_1.reshape(len(labels_1), 1),\n",
        "                         labels_2.reshape(len(labels_2), 1)), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMs4G46dkmY0"
      },
      "outputs": [],
      "source": [
        "train_images, train_labels, train_dict = images[train_indices], labels[train_indices], total_features[train_indices]\n",
        "test_images, test_labels, test_dict = images[test_indices], labels[test_indices], total_features[test_indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2tXgRld2Sie"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "# Модель сугубо нейронной сети\n",
        "@dataclass\n",
        "class SimpleCNNModel:\n",
        "    n_epochs: int = 5\n",
        "    n_row: int = 200\n",
        "    n_col: int = 200\n",
        "    input_channels: int = 1\n",
        "    random_seed: int = 1234567890\n",
        "    n_dict_features: int = 30\n",
        "    n_trait: int = 2\n",
        "\n",
        "    data_train: np.ndarray = np.ndarray([])\n",
        "    features_train: np.ndarray = np.asarray([])\n",
        "    labels_train: np.ndarray = np.ndarray([])\n",
        "\n",
        "    data_test: np.ndarray = np.asarray([])\n",
        "    features_test: np.ndarray = np.asarray([])\n",
        "    labels_test: np.ndarray = np.asarray([])\n",
        "\n",
        "    def build(self, hp: dict):\n",
        "        \"\"\"\n",
        "        Функция построения модели нейросети с функциональным интерфейсом keras\n",
        "\n",
        "        :param hp: набор гиперпараметров, отвечающих за конфигурация нейросети\n",
        "        :return: граф-представление нейросети\n",
        "        \"\"\"\n",
        "\n",
        "        inp_node = Input((self.n_row, self.n_col, self.input_channels), name=\"img_input\")\n",
        "\n",
        "        inp_node_dict = Input({self.n_dict_features}, name=\"dict_input\")\n",
        "\n",
        "        conv_node_1 = Conv2D(hp['first_conv2d_out_channels'],\n",
        "                             kernel_size=(hp['first_conv2d_kernel_size'], hp['first_conv2d_kernel_size']),\n",
        "                             padding='same',\n",
        "                             strides=(1, 1),\n",
        "                             activation=hp['first_conv2d_activation'], name=\"conv_map_1\")(inp_node)\n",
        "        if hp['need_extra_conv2d']:\n",
        "            conv_node_1 = Conv2D(hp['extra_conv2d_out_channels'],\n",
        "                                 kernel_size=(hp['extra_conv2d_kernel_size'], hp['extra_conv2d_kernel_size']),\n",
        "                                 padding='same',\n",
        "                                 strides=(1, 1),\n",
        "                                 activation=hp['extra_conv2d_activation'], name=\"conv_map_extra\")(conv_node_1)\n",
        "\n",
        "        if hp['need_batch_norm_after_first_conv2d']:\n",
        "            batch_node_1 = BatchNormalization()(conv_node_1)\n",
        "            mp_node_1 = MaxPooling2D(pool_size=(2, 2))(batch_node_1)\n",
        "        else:\n",
        "            mp_node_1 = MaxPooling2D(pool_size=(2, 2))(conv_node_1)\n",
        "\n",
        "        conv_node_2 = Conv2D(hp['second_conv2d_out_channels'],\n",
        "                             kernel_size=(hp['second_conv2d_kernel_size'], hp['second_conv2d_kernel_size']),\n",
        "                             padding='same',\n",
        "                             strides=(1, 1),\n",
        "                             activation=hp['second_conv2d_activation'], name=\"conv_map_2\")(mp_node_1)\n",
        "\n",
        "        if hp['need_batch_norm_after_second_conv2d']:\n",
        "            batch_node_2 = BatchNormalization()(conv_node_2)\n",
        "            mp_node_2 = MaxPooling2D(pool_size=(2, 2), name=\"max_pool_map\")(batch_node_2)\n",
        "        else:\n",
        "            mp_node_2 = MaxPooling2D(pool_size=(2, 2), name=\"max_pool_map\")(conv_node_2)\n",
        "\n",
        "        if hp['need_deconv_block']:\n",
        "            deconv_node_2 = Conv2DTranspose(\n",
        "                hp['second_conv2d_out_channels'],\n",
        "                kernel_size=(hp['second_conv2d_kernel_size'], hp['second_conv2d_kernel_size']),\n",
        "                padding='same',\n",
        "                strides=(2, 2),\n",
        "                activation=hp['second_conv2d_activation'],\n",
        "                name=\"deconv_2\"\n",
        "            )(mp_node_2)\n",
        "            concat_node_2 = Concatenate(name=\"concat_2\", axis=3)([deconv_node_2, conv_node_2])\n",
        "            conv_node_deconv_2 = Conv2D(\n",
        "                hp['second_conv2d_out_channels'],\n",
        "                kernel_size=(hp['second_conv2d_kernel_size'], hp['second_conv2d_kernel_size']),\n",
        "                padding='same',\n",
        "                strides=(1, 1),\n",
        "                activation=hp['second_conv2d_activation'],\n",
        "                name=\"conv_deconv_2\"\n",
        "            )(concat_node_2)\n",
        "            deconv_node_1 = Conv2DTranspose(\n",
        "                hp['first_conv2d_out_channels'],\n",
        "                kernel_size=(hp['first_conv2d_kernel_size'], hp['first_conv2d_kernel_size']),\n",
        "                padding='same',\n",
        "                strides=(2, 2),\n",
        "                activation=hp['first_conv2d_activation'],\n",
        "                name=\"deconv_1\"\n",
        "            )(conv_node_deconv_2)\n",
        "            concat_node_1 = Concatenate(name=\"concat_1\", axis=3)([deconv_node_1, conv_node_1])\n",
        "            mp_node_2 = Conv2D(\n",
        "                hp['first_conv2d_out_channels'],\n",
        "                kernel_size=(hp['first_conv2d_kernel_size'], hp['first_conv2d_kernel_size']),\n",
        "                padding='same',\n",
        "                strides=(1, 1),\n",
        "                activation=hp['first_conv2d_activation'],\n",
        "                name=\"conv_deconv_1\"\n",
        "            )(concat_node_1)\n",
        "\n",
        "        if hp['use_gap_1_or_flatten_0'] == 0:\n",
        "            flatten_node = Flatten(name='flatten')(mp_node_2)\n",
        "            dense_node = Dense(hp['num_feature_output'], activation=hp['dense_output_activation'],\n",
        "                               name=\"img_feature_output\")(flatten_node)\n",
        "        elif hp['use_gap_1_or_flatten_0'] == 1:\n",
        "            dense_node = GlobalAveragePooling2D(name=\"img_feature_output\")(mp_node_2)\n",
        "\n",
        "        concatenate_features = Concatenate(name=\"concat_features\")([inp_node_dict, dense_node])\n",
        "\n",
        "        out = Dense(self.n_trait, activation='linear', name=\"cnn_multioutput\")(concatenate_features)\n",
        "\n",
        "        model = Model(inputs=[inp_node, inp_node_dict], outputs=out, name=\"regression_model\")\n",
        "\n",
        "        return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5t8EVBx1_ZP"
      },
      "outputs": [],
      "source": [
        "class ComboDataPool(keras.utils.Sequence):\n",
        "\n",
        "    def __init__(self, images, features, labels, batch_size: int, max_len: int = -1):\n",
        "        self.batch_size = batch_size\n",
        "        self.images = images[:max_len]\n",
        "        self.features = features[:max_len]\n",
        "        self.labels = labels[:max_len]\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(self.images.shape[0] / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_data = [self.images[idx * self.batch_size:(idx + 1) * self.batch_size],\n",
        "                   self.features[idx * self.batch_size:(idx + 1) * self.batch_size]]\n",
        "        batch_labels = self.labels[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "\n",
        "        return batch_data, batch_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjUvQz9x2OgG"
      },
      "outputs": [],
      "source": [
        "from itertools import product, islice\n",
        "from sklearn.model_selection import KFold\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "\n",
        "class ComboModelTuner:\n",
        "\n",
        "    @staticmethod\n",
        "    @tf.function\n",
        "    def fit_loss_mae(y_true: np.ndarray, y_pred: np.ndarray) -> np.array:\n",
        "        \"\"\"\n",
        "        MAE-функция потерь для обучения при помощи стандартного метода '.fit()'\n",
        "        \"\"\"\n",
        "        error = y_true - y_pred\n",
        "        abs_error_1, abs_error_2 = tf.abs(error[:, 0]), tf.abs(error[:, 1])\n",
        "        result_1, result_2 = tf.reduce_mean(abs_error_1), tf.reduce_mean(abs_error_2)\n",
        "        return (result_1 + result_2) / 2\n",
        "\n",
        "    @staticmethod\n",
        "    @tf.function\n",
        "    def fit_loss_mse(y_true: np.ndarray, y_pred: np.ndarray) -> np.array:\n",
        "        \"\"\"\n",
        "        MSE-функция потерь для обучения при помощи стандартного метода '.fit()'\n",
        "        \"\"\"\n",
        "        error = y_true - y_pred\n",
        "        squared_error_1, squared_error_2 = tf.square(error), tf.square(error[:, 1])\n",
        "        result_1, result_2 = tf.reduce_mean(squared_error_1), tf.reduce_mean(squared_error_2)\n",
        "        return (result_1 + result_2) / 2\n",
        "\n",
        "    @staticmethod\n",
        "    @tf.function\n",
        "    def custom_loss_mae(y_true: np.ndarray, y_pred: np.ndarray) -> np.array:\n",
        "        \"\"\"\n",
        "        MAE-функция потерь для обучения при помощи пользовательской реализации цикла обучения\n",
        "        \"\"\"\n",
        "        error = y_true - y_pred\n",
        "        abs_error_1, abs_error_2 = tf.abs(error[:, 0]), tf.abs(error[:, 1])\n",
        "        result_1, result_2 = tf.reduce_mean(abs_error_1), tf.reduce_mean(abs_error_2)\n",
        "        return (result_1 + result_2) / 2\n",
        "\n",
        "    @staticmethod\n",
        "    @tf.function\n",
        "    def custom_loss_mse(y_true: np.ndarray, y_pred: np.ndarray) -> np.array:\n",
        "        \"\"\"\n",
        "        MSE-функция потерь для обучения при помощи пользовательской реализации цикла обучения\n",
        "        \"\"\"\n",
        "        error = y_true - y_pred\n",
        "        squared_error_1, squared_error_2 = tf.square(error), tf.square(error[:, 1])\n",
        "        result_1, result_2 = tf.reduce_mean(squared_error_1), tf.reduce_mean(squared_error_2)\n",
        "        return (result_1 + result_2) / 2\n",
        "\n",
        "    @staticmethod\n",
        "    def custom_cv(total_hp: dict, cnn_hp: dict, model: 'SimpleCNNModel',\n",
        "                  splits_num: int = 10,\n",
        "                  early_stop: bool = True, model_checkpoint: bool = True,\n",
        "                  data_generator: bool = False) -> list:\n",
        "        model_keras = model.build(cnn_hp)\n",
        "\n",
        "        learning_data_pool = ComboDataPool(images=train_images,\n",
        "                                           features=train_dict,\n",
        "                                           labels=train_labels,\n",
        "                                           batch_size=64)\n",
        "\n",
        "        callbacks = []\n",
        "        if early_stop:\n",
        "            callback_early_stop = EarlyStopping(monitor=\"loss\", min_delta=0.001, patience=2, verbose=1)\n",
        "            callbacks.append(callback_early_stop)\n",
        "        if model_checkpoint:\n",
        "            callback_checkpoint = ModelCheckpoint(filepath=\"checkpoints/model_no_df_{epoch:02d}-{val_loss:.2f}.keras\",\n",
        "                                                  save_best_only=True, monitor=\"loss\", verbose=1)\n",
        "            callbacks.append(callback_checkpoint)\n",
        "\n",
        "        model_keras.compile(optimizer=keras.optimizers.SGD(learning_rate=0.0001),\n",
        "                            loss=ComboModelTuner.fit_loss_mae,\n",
        "                            metrics=[ComboModelTuner.fit_loss_mse])\n",
        "\n",
        "        learning_data_pool = ComboDataPool(images=train_images,\n",
        "                                           features=train_dict,\n",
        "                                           labels=train_labels,\n",
        "                                           batch_size=64)\n",
        "\n",
        "        mae_per_fold_tr, mse_per_fold_tr = [], []\n",
        "        mae_per_fold_vd, mse_per_fold_vd = [], []\n",
        "\n",
        "        kfold = KFold(n_splits=splits_num, shuffle=True)\n",
        "        for j, (tr_idx, val_idx) in enumerate(kfold.split(model.features_train, model.data_train, model.labels_train)):\n",
        "            if data_generator:\n",
        "                history = model_keras.fit(learning_data_pool, epochs=total_hp[\"num_epochs_ll\"],\n",
        "                                      validation_data=([model.data_train[val_idx], model.features_train[val_idx]],\n",
        "                                                      model.labels_train[val_idx]))\n",
        "            else:\n",
        "                history = model_keras.fit(x=[model.data_train[tr_idx], model.features_train[tr_idx]],\n",
        "                                          y=model.labels_train[tr_idx],\n",
        "                                          batch_size=total_hp[\"batch_size_ll\"],\n",
        "                                          epochs=total_hp[\"num_epochs_ll\"],\n",
        "                                          validation_data=([model.data_train[val_idx], model.features_train[val_idx]],\n",
        "                                                          model.labels_train[val_idx]),\n",
        "                                          callbacks=callbacks)\n",
        "\n",
        "            scores = model_keras.evaluate(x=[model.data_train[tr_idx], model.features_train[tr_idx]],\n",
        "                                          y=model.labels_train[tr_idx])\n",
        "\n",
        "            mse_per_fold_tr.append(scores[0])\n",
        "            mae_per_fold_tr.append(scores[1])\n",
        "\n",
        "            scores = model_keras.evaluate(x=[model.data_train[val_idx], model.features_train[val_idx]],\n",
        "                                          y=model.labels_train[val_idx])\n",
        "\n",
        "            mse_per_fold_vd.append(scores[0])\n",
        "            mae_per_fold_vd.append(scores[1])\n",
        "            print(f\"Fold #{j + 1} finished succesfully\")\n",
        "\n",
        "        return [mae_per_fold_tr, mse_per_fold_tr, mae_per_fold_vd, mse_per_fold_vd], model_keras\n",
        "\n",
        "    @staticmethod\n",
        "    def fit_cv(total_hp: dict, cnn_hp: dict, model: 'SimpleCNNModel',\n",
        "               splits_num: int = 10,\n",
        "               early_stop: bool = True, model_checkpoint: bool = True,\n",
        "               data_generator: bool = False) -> list:\n",
        "        model_keras = model.build(cnn_hp)\n",
        "\n",
        "        learning_data_pool = ComboDataPool(images=train_images,\n",
        "                                           features=train_dict,\n",
        "                                           labels=train_labels,\n",
        "                                           batch_size=64)\n",
        "\n",
        "        callbacks = []\n",
        "        if early_stop:\n",
        "            callback_early_stop = EarlyStopping(monitor=\"loss\", min_delta=0.001, patience=2, verbose=1)\n",
        "            callbacks.append(callback_early_stop)\n",
        "        if model_checkpoint:\n",
        "            callback_checkpoint = ModelCheckpoint(filepath=\"checkpoints/model_no_df_{epoch:02d}-{val_loss:.2f}.keras\",\n",
        "                                                  save_best_only=True, monitor=\"loss\", verbose=1)\n",
        "            callbacks.append(callback_checkpoint)\n",
        "\n",
        "        model_keras.compile(optimizer=keras.optimizers.SGD(learning_rate=0.0001),\n",
        "                            loss=ComboModelTuner.custom_loss_mae,\n",
        "                            metrics=[ComboModelTuner.custom_loss_mse])\n",
        "\n",
        "        mae_per_fold_tr, mse_per_fold_tr = [], []\n",
        "        mae_per_fold_vd, mse_per_fold_vd = [], []\n",
        "\n",
        "        mae_per_fold_tr_epochend, mse_per_fold_tr_epochend = [], []\n",
        "        mae_per_fold_vd_epochend, mse_per_fold_vd_epochend = [], []\n",
        "\n",
        "        kfold = KFold(n_splits=splits_num, shuffle=True)\n",
        "        for j, (tr_idx, val_idx) in enumerate(kfold.split(model.features_train, model.data_train, model.labels_train)):\n",
        "            if data_generator:\n",
        "                history = model_keras.fit(learning_data_pool, epochs=total_hp[\"num_epochs_ll\"],\n",
        "                                          validation_data=([model.data_train[val_idx], model.features_train[val_idx]],\n",
        "                                                          model.labels_train[val_idx]))\n",
        "            else:\n",
        "                history = model_keras.fit(x=[model.data_train[tr_idx], model.features_train[tr_idx]],\n",
        "                                          y=model.labels_train[tr_idx],\n",
        "                                          batch_size=total_hp[\"batch_size_ll\"],\n",
        "                                          epochs=total_hp[\"num_epochs_ll\"],\n",
        "                                          validation_data=([model.data_train[val_idx], model.features_train[val_idx]],\n",
        "                                                          model.labels_train[val_idx]),\n",
        "                                          callbacks=callbacks)\n",
        "\n",
        "            scores = model_keras.evaluate(x=[model.data_train[tr_idx], model.features_train[tr_idx]],\n",
        "                                          y=model.labels_train[tr_idx])\n",
        "\n",
        "            mae_per_fold_tr_epochend.append(scores[0])\n",
        "            mse_per_fold_tr_epochend.append(scores[1])\n",
        "\n",
        "            scores = model_keras.evaluate(x=[model.data_train[val_idx], model.features_train[val_idx]],\n",
        "                                          y=model.labels_train[val_idx])\n",
        "\n",
        "            mae_per_fold_vd_epochend.append(scores[0])\n",
        "            mse_per_fold_vd_epochend.append(scores[1])\n",
        "\n",
        "            tmp_mae_tr = history.history[\"loss\"]\n",
        "            tmp_mse_tr = history.history[\"custom_loss_mse\"]\n",
        "            tmp_mae_val = history.history[\"val_loss\"]\n",
        "            tmp_mse_val = history.history[\"val_custom_loss_mse\"]\n",
        "\n",
        "            mae_per_fold_tr = mse_per_fold_tr + tmp_mae_tr\n",
        "            mse_per_fold_tr = mae_per_fold_tr + tmp_mae_tr\n",
        "            mae_per_fold_vd = mae_per_fold_vd + tmp_mae_val\n",
        "            mse_per_fold_vd = mse_per_fold_vd + tmp_mse_val\n",
        "            print(f\"Fold #{j + 1} finished succesfully\")\n",
        "\n",
        "        return [[mae_per_fold_tr_epochend, mse_per_fold_tr_epochend, mae_per_fold_vd_epochend, mse_per_fold_vd_epochend],\n",
        "                [mae_per_fold_tr, mse_per_fold_tr, mae_per_fold_vd, mse_per_fold_vd]], model_keras\n",
        "\n",
        "    @staticmethod\n",
        "    def random_hyper_tuning(iters_num: int, hps_cnn: dict,\n",
        "                            train_images_: np.ndarray, train_features_: np.ndarray, train_labels_: np.ndarray,\n",
        "                            test_images_: np.ndarray, test_features_: np.ndarray, test_labels_: np.ndarray,\n",
        "                            early_stop: bool = True, model_checkpoint: bool = False,\n",
        "                            save_folder_name: str = \"gg\"):\n",
        "\n",
        "        test_mae, test_mse = [], []\n",
        "\n",
        "        fl_mse_train_epochend = open(f\"metrics/{save_folder_name}/mse_trains_{save_folder_name}_epochend.pickle\", \"wb\")\n",
        "        fl_mae_train_epochend = open(f\"metrics/{save_folder_name}/mae_trains_{save_folder_name}_epochend.pickle\", \"wb\")\n",
        "        fl_mse_valid_epochend = open(f\"metrics/{save_folder_name}/mse_valid_{save_folder_name}_epochend.pickle\", \"wb\")\n",
        "        fl_mae_valid_epochend = open(f\"metrics/{save_folder_name}/mae_valid_{save_folder_name}_epochend.pickle\", \"wb\")\n",
        "\n",
        "        fl_mse_train = open(f\"metrics/{save_folder_name}/mse_trains_{save_folder_name}.pickle\", \"wb\")\n",
        "        fl_mae_train = open(f\"metrics/{save_folder_name}/mae_trains_{save_folder_name}.pickle\", \"wb\")\n",
        "        fl_mse_valid = open(f\"metrics/{save_folder_name}/mse_valid_{save_folder_name}.pickle\", \"wb\")\n",
        "        fl_mae_valid = open(f\"metrics/{save_folder_name}/mae_valid_{save_folder_name}.pickle\", \"wb\")\n",
        "\n",
        "        for iter_ in range(iters_num):\n",
        "            # Сборка комбинации случайных гиперпараметров в заданын границах\n",
        "            print(f\"Random Tuning iter #{iter_} started\")\n",
        "\n",
        "            cnn_hp_comb = {}\n",
        "\n",
        "            for param in hps_cnn:\n",
        "                if len(hps_cnn[param]) > 1:\n",
        "                    if any(isinstance(x, bool) for x in hps_cnn[param]) or \\\n",
        "                            any(isinstance(x, str) for x in hps_cnn[param]):\n",
        "                        cnn_hp_comb[param] = hps_cnn[param][np.random.randint(len(hps_cnn[param]))]\n",
        "                    else:\n",
        "                        cnn_hp_comb[param] = np.random.randint(low=min(hps_cnn[param]), high=max(hps_cnn[param]))\n",
        "                else:\n",
        "                    cnn_hp_comb[param] = hps_cnn[param][0]\n",
        "\n",
        "            print(cnn_hp_comb)\n",
        "\n",
        "            num_epochs = 5\n",
        "\n",
        "            model = SimpleCNNModel(n_epochs=num_epochs,\n",
        "                                   n_row=200,\n",
        "                                   n_col=200,\n",
        "                                   input_channels=1,\n",
        "                                   random_seed=1234567890,\n",
        "                                   n_dict_features=7,\n",
        "                                   n_trait=2,\n",
        "                                   data_train=train_images_,\n",
        "                                   labels_train=train_labels_,\n",
        "                                   features_train=train_features_,\n",
        "                                   data_test=test_images_,\n",
        "                                   features_test=test_features_,\n",
        "                                   labels_test=test_labels_)\n",
        "\n",
        "            metrics, model = ComboModelTuner.fit_cv(total_hp={\"batch_size_ll\": 64, \"num_epochs_ll\": num_epochs},\n",
        "                                                    cnn_hp=cnn_hp_comb,\n",
        "                                                    splits_num=5,\n",
        "                                                    model=model,\n",
        "                                                    early_stop=early_stop,\n",
        "                                                    model_checkpoint=model_checkpoint)\n",
        "\n",
        "            model.save(f\"/content/model_saves/{save_folder_name}/rand_cv_trained_model_iter{iter_}.keras\")\n",
        "\n",
        "            pickle.dump(metrics[0][0], fl_mae_train_epochend)\n",
        "            pickle.dump(metrics[0][1], fl_mse_train_epochend)\n",
        "            pickle.dump(metrics[0][2], fl_mae_valid_epochend)\n",
        "            pickle.dump(metrics[0][3], fl_mse_valid_epochend)\n",
        "\n",
        "            pickle.dump(metrics[1][0], fl_mae_train)\n",
        "            pickle.dump(metrics[1][1], fl_mse_train)\n",
        "            pickle.dump(metrics[1][2], fl_mae_valid)\n",
        "            pickle.dump(metrics[1][3], fl_mse_valid)\n",
        "\n",
        "            # считаем ошибку модели на тестовой выборке\n",
        "            scores_test = model.predict([test_images_, test_features_], test_labels_)\n",
        "\n",
        "            test_mae.append(scores_test[0])\n",
        "            test_mse.append(scores_test[1])\n",
        "\n",
        "            print(f\"Random Tuning iter #{iter_} finished successfully\")\n",
        "\n",
        "        with open(f\"metrics/{save_folder_name}/mae_test_{save_folder_name}.pickle\", \"wb\") as fl:\n",
        "            pickle.dump(test_mae, fl)\n",
        "        with open(f\"metrics/{save_folder_name}/mse_test_{save_folder_name}.pickle\", \"wb\") as fl:\n",
        "            pickle.dump(test_mse, fl)\n",
        "\n",
        "    @staticmethod\n",
        "    def grid_hyper_tuning(iters_num: int, hps_cnn: dict,\n",
        "                          train_images_: np.ndarray, train_features_: np.ndarray, train_labels_: np.ndarray,\n",
        "                          test_images_: np.ndarray, test_features_: np.ndarray, test_labels_: np.ndarray,\n",
        "                          early_stop: bool = True, model_checkpoint: bool = False,\n",
        "                          save_folder_name: str = \"gg\"):\n",
        "        cnn_hp_combos = (dict(zip(hps_cnn.keys(), values)) for values in product(*hps_cnn.values()))\n",
        "\n",
        "        test_mae, test_mse = [], []\n",
        "\n",
        "        fl_mse_train_epochend = open(f\"metrics/{save_folder_name}/mse_trains_{save_folder_name}_epochend.pickle\", \"wb\")\n",
        "        fl_mae_train_epochend = open(f\"metrics/{save_folder_name}/mae_trains_{save_folder_name}_epochend.pickle\", \"wb\")\n",
        "        fl_mse_valid_epochend = open(f\"metrics/{save_folder_name}/mse_valid_{save_folder_name}_epochend.pickle\", \"wb\")\n",
        "        fl_mae_valid_epochend = open(f\"metrics/{save_folder_name}/mae_valid_{save_folder_name}_epochend.pickle\", \"wb\")\n",
        "\n",
        "        fl_mse_train = open(f\"metrics/{save_folder_name}/mse_trains_{save_folder_name}.pickle\", \"wb\")\n",
        "        fl_mae_train = open(f\"metrics/{save_folder_name}/mae_trains_{save_folder_name}.pickle\", \"wb\")\n",
        "        fl_mse_valid = open(f\"metrics/{save_folder_name}/mse_valid_{save_folder_name}.pickle\", \"wb\")\n",
        "        fl_mae_valid = open(f\"metrics/{save_folder_name}/mae_valid_{save_folder_name}.pickle\", \"wb\")\n",
        "\n",
        "        for i, tmp_hps_cnn in enumerate(islice(cnn_hp_combos, iters_num)):\n",
        "\n",
        "            num_epochs = 5\n",
        "\n",
        "            tmp_hps_cnn\n",
        "\n",
        "            model = SimpleCNNModel(n_epochs=num_epochs,\n",
        "                                   n_row=200,\n",
        "                                   n_col=200,\n",
        "                                   input_channels=1,\n",
        "                                   random_seed=1234567890,\n",
        "                                   n_dict_features=7,\n",
        "                                   n_trait=2,\n",
        "                                   data_train=train_images_,\n",
        "                                   labels_train=train_labels_,\n",
        "                                   features_train=train_features_,\n",
        "                                   data_test=test_images_,\n",
        "                                   features_test=test_features_,\n",
        "                                   labels_test=test_labels_)\n",
        "\n",
        "            metrics, model = ComboModelTuner.fit_cv(total_hp={\"batch_size_ll\": 64, \"num_epochs_ll\": num_epochs},\n",
        "                                                    cnn_hp=tmp_hps_cnn,\n",
        "                                                    splits_num=5,\n",
        "                                                    model=model,\n",
        "                                                    early_stop=early_stop,\n",
        "                                                    model_checkpoint=model_checkpoint)\n",
        "\n",
        "            pickle.dump(metrics[0][0], fl_mae_train_epochend)\n",
        "            pickle.dump(metrics[0][1], fl_mse_train_epochend)\n",
        "            pickle.dump(metrics[0][2], fl_mae_valid_epochend)\n",
        "            pickle.dump(metrics[0][3], fl_mse_valid_epochend)\n",
        "\n",
        "            pickle.dump(metrics[1][0], fl_mae_train)\n",
        "            pickle.dump(metrics[1][1], fl_mse_train)\n",
        "            pickle.dump(metrics[1][2], fl_mae_valid)\n",
        "            pickle.dump(metrics[1][3], fl_mse_valid)\n",
        "\n",
        "            model.save(f\"/content/model_saves/{save_folder_name}/grid_cv_trained_model_iter{i}.keras\")\n",
        "\n",
        "            # считаем ошибку модели на тестовой выборке\n",
        "            scores_test = model.evaluate([test_images_, test_features_], test_labels_)\n",
        "\n",
        "            test_mae.append(scores_test[0])\n",
        "            test_mse.append(scores_test[1])\n",
        "            print(f\"Grid Tuning iter #{i + 1} finished successfully\")\n",
        "\n",
        "        with open(f\"metrics/{save_folder_name}/mae_test_{save_folder_name}.pickle\", \"wb\") as fl:\n",
        "            pickle.dump(test_mae, fl)\n",
        "        with open(f\"metrics/{save_folder_name}/mse_test_{save_folder_name}.pickle\", \"wb\") as fl:\n",
        "            pickle.dump(test_mse, fl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FzfGemud2HsX",
        "outputId": "69c9d3cd-c2db-4712-ca23-95c3931564cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "3/3 [==============================] - 16s 5s/step - loss: 181.4627 - custom_loss_mse: 43967.5469 - val_loss: 117.1712 - val_custom_loss_mse: 16394.2637\n",
            "Epoch 2/5\n",
            "3/3 [==============================] - 15s 5s/step - loss: 98.2670 - custom_loss_mse: 12729.6611 - val_loss: 63.6419 - val_custom_loss_mse: 4650.6450\n",
            "Epoch 3/5\n",
            "3/3 [==============================] - 14s 5s/step - loss: 53.3808 - custom_loss_mse: 3534.6428 - val_loss: 38.9850 - val_custom_loss_mse: 1944.6111\n",
            "Epoch 4/5\n",
            "3/3 [==============================] - 15s 5s/step - loss: 42.0999 - custom_loss_mse: 2201.0242 - val_loss: 39.2016 - val_custom_loss_mse: 2828.5479\n",
            "Epoch 5/5\n",
            "3/3 [==============================] - 14s 5s/step - loss: 38.0140 - custom_loss_mse: 2515.5032 - val_loss: 46.0651 - val_custom_loss_mse: 2606.2920\n",
            "5/5 [==============================] - 6s 1s/step - loss: 43.5916 - custom_loss_mse: 2632.0562\n",
            "2/2 [==============================] - 1s 85ms/step - loss: 46.0651 - custom_loss_mse: 2409.0151\n",
            "Fold #1 finished succesfully\n",
            "Epoch 1/5\n",
            "3/3 [==============================] - 15s 5s/step - loss: 36.0335 - custom_loss_mse: 2409.1858 - val_loss: 22.7515 - val_custom_loss_mse: 832.9725\n",
            "Epoch 2/5\n",
            "3/3 [==============================] - 15s 5s/step - loss: 31.9968 - custom_loss_mse: 1938.8896 - val_loss: 36.4546 - val_custom_loss_mse: 1869.6309\n",
            "Epoch 3/5\n",
            "3/3 [==============================] - 15s 5s/step - loss: 36.2471 - custom_loss_mse: 1951.9673 - val_loss: 42.6972 - val_custom_loss_mse: 3016.1162\n",
            "Epoch 4/5\n",
            "3/3 [==============================] - 15s 5s/step - loss: 39.1101 - custom_loss_mse: 2312.2734 - val_loss: 48.3139 - val_custom_loss_mse: 3582.3955\n",
            "Epoch 4: early stopping\n",
            "5/5 [==============================] - 3s 677ms/step - loss: 43.0965 - custom_loss_mse: 2805.6736\n",
            "2/2 [==============================] - 1s 83ms/step - loss: 48.3139 - custom_loss_mse: 2399.3660\n",
            "Fold #2 finished succesfully\n",
            "Epoch 1/5\n",
            "3/3 [==============================] - 15s 4s/step - loss: 36.0969 - custom_loss_mse: 3068.0322 - val_loss: 47.6136 - val_custom_loss_mse: 3423.5845\n",
            "Epoch 2/5\n",
            "3/3 [==============================] - 14s 4s/step - loss: 36.3051 - custom_loss_mse: 3364.3914 - val_loss: 14.4515 - val_custom_loss_mse: 345.9684\n",
            "Epoch 3/5\n",
            "3/3 [==============================] - 15s 5s/step - loss: 33.2998 - custom_loss_mse: 1741.0343 - val_loss: 55.4744 - val_custom_loss_mse: 4824.9150\n",
            "Epoch 4/5\n",
            "3/3 [==============================] - 14s 5s/step - loss: 39.9391 - custom_loss_mse: 2589.1545 - val_loss: 30.4254 - val_custom_loss_mse: 1538.4111\n",
            "Epoch 5/5\n",
            "3/3 [==============================] - 14s 5s/step - loss: 36.5923 - custom_loss_mse: 1936.5551 - val_loss: 41.5872 - val_custom_loss_mse: 2571.3054\n",
            "Epoch 5: early stopping\n",
            "5/5 [==============================] - 4s 670ms/step - loss: 40.3581 - custom_loss_mse: 2348.6587\n",
            "2/2 [==============================] - 1s 94ms/step - loss: 41.5872 - custom_loss_mse: 2215.6191\n",
            "Fold #3 finished succesfully\n",
            "Epoch 1/5\n",
            "3/3 [==============================] - 17s 6s/step - loss: 39.6440 - custom_loss_mse: 2132.9744 - val_loss: 52.8387 - val_custom_loss_mse: 3872.1099\n",
            "Epoch 2/5\n",
            "3/3 [==============================] - 15s 5s/step - loss: 38.8350 - custom_loss_mse: 2244.4592 - val_loss: 29.5528 - val_custom_loss_mse: 1294.2988\n",
            "Epoch 3/5\n",
            "3/3 [==============================] - 15s 5s/step - loss: 35.5908 - custom_loss_mse: 2402.4441 - val_loss: 38.9143 - val_custom_loss_mse: 2308.5776\n",
            "Epoch 4/5\n",
            "3/3 [==============================] - 15s 5s/step - loss: 35.5935 - custom_loss_mse: 1708.8602 - val_loss: 68.7780 - val_custom_loss_mse: 8108.2031\n",
            "Epoch 5/5\n",
            "3/3 [==============================] - 14s 5s/step - loss: 43.3256 - custom_loss_mse: 3372.2903 - val_loss: 44.6681 - val_custom_loss_mse: 3454.7781\n",
            "Epoch 5: early stopping\n",
            "5/5 [==============================] - 4s 684ms/step - loss: 48.9844 - custom_loss_mse: 4080.7437\n",
            "2/2 [==============================] - 1s 85ms/step - loss: 44.6681 - custom_loss_mse: 3686.3364\n",
            "Fold #4 finished succesfully\n",
            "Epoch 1/5\n",
            "3/3 [==============================] - 14s 5s/step - loss: 39.1148 - custom_loss_mse: 2988.4866 - val_loss: 45.1185 - val_custom_loss_mse: 2592.2354\n",
            "Epoch 2/5\n",
            "3/3 [==============================] - 15s 5s/step - loss: 40.0479 - custom_loss_mse: 2292.8176 - val_loss: 17.6499 - val_custom_loss_mse: 600.8727\n",
            "Epoch 3/5\n",
            "3/3 [==============================] - 14s 4s/step - loss: 32.1035 - custom_loss_mse: 1895.5592 - val_loss: 35.1239 - val_custom_loss_mse: 1673.1031\n",
            "Epoch 4/5\n",
            "3/3 [==============================] - 15s 5s/step - loss: 36.2697 - custom_loss_mse: 2179.1943 - val_loss: 25.6647 - val_custom_loss_mse: 1208.9866\n",
            "Epoch 5/5\n",
            "3/3 [==============================] - 14s 4s/step - loss: 37.6373 - custom_loss_mse: 1941.4971 - val_loss: 28.3424 - val_custom_loss_mse: 1283.0559\n",
            "Epoch 5: early stopping\n",
            "5/5 [==============================] - 4s 674ms/step - loss: 33.1859 - custom_loss_mse: 1583.2424\n",
            "2/2 [==============================] - 1s 93ms/step - loss: 28.3424 - custom_loss_mse: 753.0974\n",
            "Fold #5 finished succesfully\n",
            "1/1 [==============================] - 1s 521ms/step - loss: 28.3107 - custom_loss_mse: 1078.3560\n",
            "Grid Tuning iter #1 finished successfully\n",
            "Epoch 1/5\n",
            "3/3 [==============================] - 15s 4s/step - loss: 331.0609 - custom_loss_mse: 152242.2969 - val_loss: 241.5319 - val_custom_loss_mse: 83236.7656\n",
            "Epoch 2/5\n",
            "3/3 [==============================] - 17s 6s/step - loss: 214.2094 - custom_loss_mse: 70887.7578 - val_loss: 152.1413 - val_custom_loss_mse: 42179.2109\n",
            "Epoch 3/5\n",
            "3/3 [==============================] - 14s 5s/step - loss: 136.4353 - custom_loss_mse: 33118.2070 - val_loss: 94.4341 - val_custom_loss_mse: 18369.4512\n",
            "Epoch 4/5\n",
            "3/3 [==============================] - 15s 5s/step - loss: 79.3022 - custom_loss_mse: 13601.3779 - val_loss: 47.4223 - val_custom_loss_mse: 6131.2705\n",
            "Epoch 5/5\n",
            "3/3 [==============================] - 15s 5s/step - loss: 41.8624 - custom_loss_mse: 4092.2961 - val_loss: 27.0243 - val_custom_loss_mse: 1355.4027\n",
            "5/5 [==============================] - 5s 1s/step - loss: 24.4291 - custom_loss_mse: 1229.1476\n",
            "2/2 [==============================] - 1s 79ms/step - loss: 27.0243 - custom_loss_mse: 1044.4202\n",
            "Fold #1 finished succesfully\n",
            "Epoch 1/5\n",
            "3/3 [==============================] - 15s 5s/step - loss: 30.7379 - custom_loss_mse: 1655.1735 - val_loss: 46.1999 - val_custom_loss_mse: 2805.8188\n",
            "Epoch 2/5\n",
            "3/3 [==============================] - 15s 5s/step - loss: 38.6519 - custom_loss_mse: 2028.8119 - val_loss: 22.4607 - val_custom_loss_mse: 983.5332\n",
            "Epoch 3/5\n",
            "3/3 [==============================] - 15s 5s/step - loss: 31.4395 - custom_loss_mse: 2118.6309 - val_loss: 35.9446 - val_custom_loss_mse: 2020.4448\n",
            "Epoch 3: early stopping\n",
            "5/5 [==============================] - 4s 671ms/step - loss: 31.5882 - custom_loss_mse: 1650.7292\n",
            "2/2 [==============================] - 1s 83ms/step - loss: 35.9446 - custom_loss_mse: 1349.2908\n",
            "Fold #2 finished succesfully\n",
            "Epoch 1/5\n",
            "3/3 [==============================] - 16s 5s/step - loss: 37.7407 - custom_loss_mse: 2876.5774 - val_loss: 49.0069 - val_custom_loss_mse: 4174.1436\n",
            "Epoch 2/5\n",
            "3/3 [==============================] - 14s 4s/step - loss: 47.0057 - custom_loss_mse: 4043.0872 - val_loss: 21.6003 - val_custom_loss_mse: 698.2887\n",
            "Epoch 3/5\n",
            "3/3 [==============================] - 15s 4s/step - loss: 33.4153 - custom_loss_mse: 2644.8416 - val_loss: 11.0884 - val_custom_loss_mse: 162.2594\n",
            "Epoch 4/5\n",
            "3/3 [==============================] - 15s 4s/step - loss: 36.0331 - custom_loss_mse: 1972.7916 - val_loss: 50.4828 - val_custom_loss_mse: 4075.8804\n",
            "Epoch 5/5\n",
            "3/3 [==============================] - 14s 4s/step - loss: 43.7828 - custom_loss_mse: 4531.1440 - val_loss: 28.6530 - val_custom_loss_mse: 1012.8466\n",
            "Epoch 5: early stopping\n",
            "5/5 [==============================] - 4s 741ms/step - loss: 40.3998 - custom_loss_mse: 1856.8600\n",
            "2/2 [==============================] - 1s 138ms/step - loss: 28.6530 - custom_loss_mse: 1463.9937\n",
            "Fold #3 finished succesfully\n",
            "Epoch 1/5\n",
            "3/3 [==============================] - 16s 5s/step - loss: 36.3059 - custom_loss_mse: 1666.7123 - val_loss: 57.2976 - val_custom_loss_mse: 6106.9316\n",
            "Epoch 2/5\n",
            "3/3 [==============================] - 16s 5s/step - loss: 36.1765 - custom_loss_mse: 3075.2424 - val_loss: 15.4013 - val_custom_loss_mse: 407.6895\n",
            "Epoch 3/5\n",
            "3/3 [==============================] - 16s 5s/step - loss: 19.6173 - custom_loss_mse: 1196.4384 - val_loss: 50.9244 - val_custom_loss_mse: 3802.6748\n",
            "Epoch 4/5\n",
            "3/3 [==============================] - 15s 5s/step - loss: 36.9945 - custom_loss_mse: 2783.6794 - val_loss: 34.0596 - val_custom_loss_mse: 2055.7212\n",
            "Epoch 5/5\n",
            "3/3 [==============================] - 15s 5s/step - loss: 31.9262 - custom_loss_mse: 2009.6742 - val_loss: 63.2905 - val_custom_loss_mse: 6421.1006\n",
            "Epoch 5: early stopping\n",
            "5/5 [==============================] - 4s 682ms/step - loss: 59.6957 - custom_loss_mse: 5302.0806\n",
            "2/2 [==============================] - 1s 85ms/step - loss: 63.2905 - custom_loss_mse: 4160.1597\n",
            "Fold #4 finished succesfully\n",
            "Epoch 1/5\n",
            "3/3 [==============================] - 16s 4s/step - loss: 36.3115 - custom_loss_mse: 3117.8145 - val_loss: 35.9045 - val_custom_loss_mse: 1923.6475\n",
            "Epoch 2/5\n",
            "3/3 [==============================] - 16s 4s/step - loss: 34.5451 - custom_loss_mse: 3108.4529 - val_loss: 48.1437 - val_custom_loss_mse: 3337.0823\n",
            "Epoch 3/5\n",
            "3/3 [==============================] - 15s 4s/step - loss: 39.1004 - custom_loss_mse: 2569.2585 - val_loss: 38.7497 - val_custom_loss_mse: 2198.4932\n",
            "Epoch 4/5\n",
            "3/3 [==============================] - 15s 5s/step - loss: 38.8011 - custom_loss_mse: 2119.4639 - val_loss: 13.0212 - val_custom_loss_mse: 237.9284\n",
            "Epoch 4: early stopping\n",
            "5/5 [==============================] - 3s 667ms/step - loss: 13.0082 - custom_loss_mse: 241.3295\n",
            "2/2 [==============================] - 1s 77ms/step - loss: 13.0212 - custom_loss_mse: 252.1746\n",
            "Fold #5 finished succesfully\n",
            "1/1 [==============================] - 1s 524ms/step - loss: 11.7050 - custom_loss_mse: 205.1974\n",
            "Grid Tuning iter #2 finished successfully\n",
            "Epoch 1/5\n",
            "3/3 [==============================] - 15s 4s/step - loss: 272.3405 - custom_loss_mse: 134894.3906 - val_loss: 179.6915 - val_custom_loss_mse: 64118.5391\n",
            "Epoch 2/5\n",
            "3/3 [==============================] - 15s 4s/step - loss: 148.2259 - custom_loss_mse: 37619.8320 - val_loss: 97.9161 - val_custom_loss_mse: 16864.3027\n",
            "Epoch 3/5\n",
            "3/3 [==============================] - 15s 4s/step - loss: 73.1950 - custom_loss_mse: 7070.1797 - val_loss: 39.1459 - val_custom_loss_mse: 2260.7808\n",
            "Epoch 4/5\n",
            "3/3 [==============================] - 15s 4s/step - loss: 38.7739 - custom_loss_mse: 2247.6130 - val_loss: 41.9315 - val_custom_loss_mse: 3284.8389\n",
            "Epoch 5/5\n",
            "3/3 [==============================] - 15s 5s/step - loss: 35.3460 - custom_loss_mse: 3672.2100 - val_loss: 34.8155 - val_custom_loss_mse: 1736.3193\n",
            "5/5 [==============================] - 4s 687ms/step - loss: 36.2707 - custom_loss_mse: 1756.0554\n",
            "2/2 [==============================] - 1s 134ms/step - loss: 34.8155 - custom_loss_mse: 2090.6819\n",
            "Fold #1 finished succesfully\n",
            "Epoch 1/5\n",
            "3/3 [==============================] - 15s 5s/step - loss: 40.5328 - custom_loss_mse: 1994.1758 - val_loss: 32.9251 - val_custom_loss_mse: 1758.1331\n",
            "Epoch 2/5\n",
            "3/3 [==============================] - 15s 5s/step - loss: 33.4815 - custom_loss_mse: 1574.8467 - val_loss: 72.8236 - val_custom_loss_mse: 8104.8232\n",
            "Epoch 3/5\n",
            "3/3 [==============================] - 14s 4s/step - loss: 42.6609 - custom_loss_mse: 3529.2471 - val_loss: 48.8205 - val_custom_loss_mse: 3343.8293\n",
            "Epoch 4/5\n",
            "3/3 [==============================] - 14s 4s/step - loss: 39.4594 - custom_loss_mse: 2278.3596 - val_loss: 43.3645 - val_custom_loss_mse: 2773.8992\n",
            "Epoch 4: early stopping\n",
            "5/5 [==============================] - 5s 968ms/step - loss: 39.4708 - custom_loss_mse: 2444.0764\n",
            "2/2 [==============================] - 1s 80ms/step - loss: 43.3645 - custom_loss_mse: 3734.2654\n",
            "Fold #2 finished succesfully\n",
            "Epoch 1/5\n",
            "3/3 [==============================] - 14s 5s/step - loss: 36.8205 - custom_loss_mse: 2258.1731 - val_loss: 43.5963 - val_custom_loss_mse: 2664.8484\n",
            "Epoch 2/5\n",
            "3/3 [==============================] - 15s 5s/step - loss: 42.2264 - custom_loss_mse: 2250.6609 - val_loss: 64.8860 - val_custom_loss_mse: 6076.0176\n",
            "Epoch 3/5\n",
            "3/3 [==============================] - 15s 5s/step - loss: 43.7034 - custom_loss_mse: 3159.2246 - val_loss: 36.5890 - val_custom_loss_mse: 1853.4745\n",
            "Epoch 3: early stopping\n",
            "5/5 [==============================] - 4s 685ms/step - loss: 35.6288 - custom_loss_mse: 1871.5833\n",
            "2/2 [==============================] - 1s 78ms/step - loss: 36.5890 - custom_loss_mse: 1343.7083\n",
            "Fold #3 finished succesfully\n",
            "Epoch 1/5\n",
            "3/3 [==============================] - 15s 5s/step - loss: 34.9799 - custom_loss_mse: 2176.4558 - val_loss: 82.3634 - val_custom_loss_mse: 10683.1016\n",
            "Epoch 2/5\n",
            "3/3 [==============================] - 15s 5s/step - loss: 41.3876 - custom_loss_mse: 3323.4753 - val_loss: 18.9314 - val_custom_loss_mse: 484.4796\n",
            "Epoch 3/5\n",
            "3/3 [==============================] - 14s 5s/step - loss: 20.7658 - custom_loss_mse: 1011.5695 - val_loss: 42.3680 - val_custom_loss_mse: 3194.2126\n",
            "Epoch 4/5\n",
            "3/3 [==============================] - 15s 5s/step - loss: 36.1821 - custom_loss_mse: 2187.7102 - val_loss: 43.0281 - val_custom_loss_mse: 2798.2881\n",
            "Epoch 5/5\n",
            "3/3 [==============================] - 16s 5s/step - loss: 33.9041 - custom_loss_mse: 2401.0161 - val_loss: 21.3204 - val_custom_loss_mse: 747.9066\n",
            "Epoch 5: early stopping\n",
            "5/5 [==============================] - 3s 677ms/step - loss: 22.2191 - custom_loss_mse: 737.8017\n",
            "2/2 [==============================] - 1s 80ms/step - loss: 21.3204 - custom_loss_mse: 838.6394\n",
            "Fold #4 finished succesfully\n",
            "Epoch 1/5\n",
            "3/3 [==============================] - 14s 4s/step - loss: 37.1141 - custom_loss_mse: 1933.3219 - val_loss: 47.7727 - val_custom_loss_mse: 3857.0137\n",
            "Epoch 2/5\n",
            "3/3 [==============================] - 15s 4s/step - loss: 37.2547 - custom_loss_mse: 3266.6917 - val_loss: 17.9184 - val_custom_loss_mse: 845.2860\n",
            "Epoch 3/5\n",
            "3/3 [==============================] - 15s 5s/step - loss: 36.8212 - custom_loss_mse: 2258.5913 - val_loss: 38.8576 - val_custom_loss_mse: 2111.8271\n",
            "Epoch 4/5\n",
            "3/3 [==============================] - 14s 5s/step - loss: 41.8230 - custom_loss_mse: 2375.4775 - val_loss: 46.7556 - val_custom_loss_mse: 3897.6460\n",
            "Epoch 5/5\n",
            "3/3 [==============================] - 15s 5s/step - loss: 38.1393 - custom_loss_mse: 3380.4539 - val_loss: 17.6615 - val_custom_loss_mse: 511.5107\n",
            "Epoch 5: early stopping\n",
            "5/5 [==============================] - 4s 693ms/step - loss: 19.7444 - custom_loss_mse: 575.0021\n",
            "2/2 [==============================] - 1s 80ms/step - loss: 17.6615 - custom_loss_mse: 485.9441\n",
            "Fold #5 finished succesfully\n",
            "1/1 [==============================] - 1s 530ms/step - loss: 20.4083 - custom_loss_mse: 553.9211\n",
            "Grid Tuning iter #3 finished successfully\n"
          ]
        }
      ],
      "source": [
        "# задаем сетку гиперпараметров для\n",
        "import pickle\n",
        "\n",
        "model_hp = {# сначала идут параметры сверточной части модели\n",
        "            'first_conv2d_out_channels': [32, 64],\n",
        "            'first_conv2d_kernel_size': [3, 5, 7],\n",
        "            'first_conv2d_activation': ['tanh', 'relu'],\n",
        "            'need_extra_conv2d': [False, True],\n",
        "            'extra_conv2d_out_channels': [32, 64],\n",
        "            'extra_conv2d_kernel_size': [3, 5, 7],\n",
        "            'extra_conv2d_activation': ['tanh', 'relu'],\n",
        "            'need_batch_norm_after_first_conv2d': [True, False],\n",
        "            'second_conv2d_kernel_size': [3, 5],\n",
        "            'second_conv2d_out_channels': [64, 128],\n",
        "            'second_conv2d_activation': ['tanh', 'relu'],\n",
        "            'need_batch_norm_after_second_conv2d': [True, False],\n",
        "            'dense_output_activation': ['sigmoid', 'linear'],\n",
        "            'use_gap_1_or_flatten_0': [1, 0],\n",
        "            'need_deconv_block': [False, True],\n",
        "            'num_feature_output': [64, 128, 256],\n",
        "        }\n",
        "\n",
        "# ComboModelTuner.random_hyper_tuning(10, model_hp,\n",
        "#                                     train_images_=train_images,\n",
        "#                                     train_labels_=train_labels,\n",
        "#                                     train_features_=train_dict,\n",
        "#                                     test_images_=test_images,\n",
        "#                                     test_labels_=test_labels,\n",
        "#                                     test_features_=test_dict,\n",
        "#                                     save_folder_name=\"height_crop_test\")\n",
        "\n",
        "ComboModelTuner.grid_hyper_tuning(3, model_hp,\n",
        "                                  train_images_=train_images,\n",
        "                                  train_labels_=train_labels,\n",
        "                                  train_features_=train_dict,\n",
        "                                  test_images_=test_images,\n",
        "                                  test_labels_=test_labels,\n",
        "                                  test_features_=test_dict,\n",
        "                                  early_stop=True,\n",
        "                                  model_checkpoint=False,\n",
        "                                  save_folder_name=\"crop_yellow_rust\")\n",
        "\n",
        "# сохраним индексы разбиения на обучающий и тестовый наборы данных. Сначала идет\n",
        "# обучающий набор (train), а затем тестовый (test)\n",
        "with open(\"/content/indices/crop_yellow_rust/train_test_split.txt\", \"wb\") as fl:\n",
        "    pickle.dump(train_indices, fl)\n",
        "    pickle.dump(test_indices, fl)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# архивирование и загрузка истории обучения модели + индексирования образцов на локальную машину\n",
        "!cd /contents\n",
        "!zip -r model_checkpoints.zip model_saves/crop_yellow_rust\n",
        "!zip -r metrics.zip metrics/crop_yellow_rust\n",
        "!zip -r train_test_indices.zip indices/crop_yellow_rust\n",
        "\n",
        "from google.colab import files\n",
        "files.download('model_checkpoints.zip')\n",
        "files.download('train_test_indices.zip')\n",
        "files.download('metrics.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "id": "kN5PrXjsoTx8",
        "outputId": "93d375ee-edbb-436f-af56-3a1b33e61d4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: cd: /contents: No such file or directory\n",
            "  adding: model_saves/crop_yellow_rust/ (stored 0%)\n",
            "  adding: model_saves/crop_yellow_rust/grid_cv_trained_model_iter0.keras (deflated 27%)\n",
            "  adding: model_saves/crop_yellow_rust/grid_cv_trained_model_iter2.keras (deflated 27%)\n",
            "  adding: model_saves/crop_yellow_rust/grid_cv_trained_model_iter1.keras (deflated 27%)\n",
            "  adding: metrics/crop_yellow_rust/ (stored 0%)\n",
            "  adding: metrics/crop_yellow_rust/mae_trains_crop_yellow_rust.pickle (deflated 66%)\n",
            "  adding: metrics/crop_yellow_rust/crop_brown_rust/ (stored 0%)\n",
            "  adding: metrics/crop_yellow_rust/crop_brown_rust/train_test_split.txt (deflated 73%)\n",
            "  adding: metrics/crop_yellow_rust/mse_trains_crop_yellow_rust_epochend.pickle (deflated 43%)\n",
            "  adding: metrics/crop_yellow_rust/mae_trains_crop_yellow_rust_epochend.pickle (deflated 44%)\n",
            "  adding: metrics/crop_yellow_rust/mse_valid_crop_yellow_rust_epochend.pickle (deflated 42%)\n",
            "  adding: metrics/crop_yellow_rust/mae_valid_crop_yellow_rust_epochend.pickle (deflated 43%)\n",
            "  adding: metrics/crop_yellow_rust/train_test_split.txt (deflated 73%)\n",
            "  adding: metrics/crop_yellow_rust/.ipynb_checkpoints/ (stored 0%)\n",
            "  adding: metrics/crop_yellow_rust/mse_test_crop_yellow_rust.pickle (deflated 19%)\n",
            "  adding: metrics/crop_yellow_rust/height_crop_test/ (stored 0%)\n",
            "  adding: metrics/crop_yellow_rust/mae_valid_crop_yellow_rust.pickle (deflated 43%)\n",
            "  adding: metrics/crop_yellow_rust/mae_test_crop_yellow_rust.pickle (deflated 19%)\n",
            "  adding: metrics/crop_yellow_rust/mse_valid_crop_yellow_rust.pickle (deflated 42%)\n",
            "  adding: metrics/crop_yellow_rust/mse_trains_crop_yellow_rust.pickle (deflated 70%)\n",
            "  adding: indices/crop_yellow_rust/ (stored 0%)\n",
            "  adding: indices/crop_yellow_rust/train_test_split.txt (deflated 73%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_0c69f15a-697b-4377-a606-5e7b40b818e7\", \"model_checkpoints.zip\", 1336460)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_2404ec1e-aded-4cf9-861b-ee0f188f0618\", \"train_test_indices.zip\", 3040)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_98ae7cb2-2105-4d12-bd2c-8287406b09d0\", \"metrics.zip\", 16472)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}